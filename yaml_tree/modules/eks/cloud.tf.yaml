terraform:
- required_providers:
  - kubectl:
      source: gavinbunney/kubectl
locals:
- azs: '${var.availability_zones != 0 ? var.availability_zones : data.aws_availability_zones.available.names}'
  secondary_azs: '${var.secondary_availability_zones != 0 ? var.secondary_availability_zones
    : data.aws_availability_zones.available.names}'
  ami: '${var.fips ? var.fips_enabled_ami : data.aws_ami.eks_worker.id}'
  eks_priv_subnets: '${var.secondary_cidr_block != "" ? aws_subnet.eks_secondary_subnet.*.id
    : aws_subnet.eks_private.*.id}'
  vpc_id: '${var.vpc_id != "" ? var.vpc_id : data.aws_vpc.the_vpc.id}'
- config-map-aws-auth: "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-auth\n\
    \  namespace: kube-system\ndata:\n  mapRoles: |\n    - rolearn: ${aws_iam_role.eks_node_role.arn}\n\
    \      username: system:node:{{EC2PrivateDNSName}}\n      groups:\n        - system:bootstrappers\n\
    \        - system:nodes\n    - rolearn: ${aws_iam_role.karpenter[0].arn} \n  \
    \    username: system:node:{{SessionName}}\n      groups:\n        - system:bootstrappers\n\
    \        - system:nodes\n        - system:node-proxier\n    - rolearn: ${var.deploy_workflow\
    \ ? module.workflow_pool[0].nodepool_role : \"\"}\n      username: system:node:{{EC2PrivateDNSName}}\n\
    \      groups:\n        - system:bootstrappers\n        - system:nodes\n    -\
    \ rolearn: ${var.deploy_jupyter ? module.jupyter_pool[0].nodepool_role : \"\"\
    }\n      username: system:node:{{EC2PrivateDNSName}}\n      groups:\n        -\
    \ system:bootstrappers\n        - system:nodes"
module:
- jupyter_pool:
    count: '${var.deploy_jupyter ? 1 : 0}'
    scale_in_protection: false
    source: ../eks-nodepool/
    ec2_keyname: ${var.ec2_keyname}
    users_policy: ${var.users_policy}
    nodepool: jupyter
    vpc_name: ${var.vpc_name}
    csoc_cidr: ${var.peering_cidr}
    eks_cluster_endpoint: ${aws_eks_cluster.eks_cluster.endpoint}
    eks_cluster_ca: ${aws_eks_cluster.eks_cluster.certificate_authority[0].data}
    eks_private_subnets: ${aws_subnet.eks_private.*.id}
    control_plane_sg: ${aws_security_group.eks_control_plane_sg.id}
    default_nodepool_sg: ${aws_security_group.eks_nodes_sg.id}
    eks_version: ${var.eks_version}
    nodepool_instance_type: ${var.jupyter_instance_type}
    kernel: ${var.kernel}
    bootstrap_script: ${var.jupyter_bootstrap_script}
    nodepool_worker_drive_size: ${var.jupyter_worker_drive_size}
    organization_name: ${var.organization_name}
    nodepool_asg_desired_capacity: ${var.jupyter_asg_desired_capacity}
    nodepool_asg_max_size: ${var.jupyter_asg_max_size}
    nodepool_asg_min_size: ${var.jupyter_asg_min_size}
    activation_id: ${var.activation_id}
    customer_id: ${var.customer_id}
    vpc_id: ${local.vpc_id}
- workflow_pool:
    count: '${var.deploy_workflow ? 1 : 0}'
    scale_in_protection: true
    source: ../eks-nodepool/
    ec2_keyname: ${var.ec2_keyname}
    users_policy: ${var.users_policy}
    nodepool: workflow
    vpc_name: ${var.vpc_name}
    csoc_cidr: ${var.peering_cidr}
    eks_cluster_endpoint: ${aws_eks_cluster.eks_cluster.endpoint}
    eks_cluster_ca: ${aws_eks_cluster.eks_cluster.certificate_authority[0].data}
    eks_private_subnets: ${local.eks_priv_subnets}
    control_plane_sg: ${aws_security_group.eks_control_plane_sg.id}
    default_nodepool_sg: ${aws_security_group.eks_nodes_sg.id}
    eks_version: ${var.eks_version}
    nodepool_instance_type: ${var.workflow_instance_type}
    kernel: ${var.kernel}
    bootstrap_script: ${var.workflow_bootstrap_script}
    nodepool_worker_drive_size: ${var.workflow_worker_drive_size}
    organization_name: ${var.organization_name}
    nodepool_asg_desired_capacity: ${var.workflow_asg_desired_capacity}
    nodepool_asg_max_size: ${var.workflow_asg_max_size}
    nodepool_asg_min_size: ${var.workflow_asg_min_size}
    activation_id: ${var.activation_id}
    customer_id: ${var.customer_id}
    vpc_id: ${local.vpc_id}
resource:
- aws_iam_role:
    eks_control_plane_role:
      name: ${var.vpc_name}_EKS_role
      assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n\
        \    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n  \
        \      \"Service\": \"eks.amazonaws.com\"\n      },\n      \"Effect\": \"\
        Allow\",\n      \"Sid\": \"\"\n    }\n  ]\n}"
- aws_iam_role_policy_attachment:
    eks-policy-AmazonEKSClusterPolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
      role: ${aws_iam_role.eks_control_plane_role.name}
- aws_iam_role_policy_attachment:
    eks-policy-AmazonEKSServicePolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSServicePolicy
      role: ${aws_iam_role.eks_control_plane_role.name}
- random_shuffle:
    az:
      count: 1
      input: ${local.azs}
      result_count: ${length(local.azs)}
- random_shuffle:
    secondary_az:
      count: 1
      input: ${local.secondary_azs}
      result_count: ${length(local.secondary_azs)}
- aws_subnet:
    eks_private:
      count: ${random_shuffle.az[0].result_count}
      vpc_id: ${local.vpc_id}
      cidr_block: '${var.workers_subnet_size == 22 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block,
        2, (1 + count.index)) : var.workers_subnet_size == 23 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block,
        3, (2 + count.index)) : cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 4, (7
        + count.index))}'
      availability_zone: ${random_shuffle.az[0].result[count.index]}
      map_public_ip_on_launch: false
      lifecycle:
      - ignore_changes:
        - ${tags}
        - ${availability_zone}
        - ${cidr_block}
      tags: '${tomap({"Name": "eks_private_${count.index}", "Environment": "${var.vpc_name}",
        "Organization": "${var.organization_name}", "kubernetes.io/cluster/${var.vpc_name}":
        "owned", "kubernetes.io/role/internal-elb": "1", "karpenter.sh/discovery":
        "${var.vpc_name}"})}'
- aws_subnet:
    eks_secondary_subnet:
      count: '${var.secondary_cidr_block != "" ? 4 : 0}'
      vpc_id: ${local.vpc_id}
      cidr_block: ${cidrsubnet(var.secondary_cidr_block, 2, count.index)}
      availability_zone: ${random_shuffle.secondary_az[0].result[count.index]}
      map_public_ip_on_launch: false
      lifecycle:
      - ignore_changes:
        - ${tags}
        - ${availability_zone}
        - ${cidr_block}
      tags: '${tomap({"Name": "eks_secondary_cidr_subnet_${count.index}", "Environment":
        "${var.vpc_name}", "Organization": "${var.organization_name}", "kubernetes.io/cluster/${var.vpc_name}":
        "owned", "kubernetes.io/role/internal-elb": "1", "karpenter.sh/discovery":
        "${var.vpc_name}"})}'
- aws_subnet:
    eks_public:
      count: ${random_shuffle.az[0].result_count}
      vpc_id: ${local.vpc_id}
      cidr_block: '${var.workers_subnet_size == 22 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block,
        5, (4 + count.index)) : cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 4, (10
        + count.index))}'
      map_public_ip_on_launch: true
      availability_zone: ${random_shuffle.az[0].result[count.index]}
      lifecycle:
      - ignore_changes:
        - ${tags}
        - ${availability_zone}
        - ${cidr_block}
      tags: '${tomap({"Name": "eks_public_${count.index}", "Environment": "${var.vpc_name}",
        "Organization": "${var.organization_name}", "kubernetes.io/cluster/${var.vpc_name}":
        "shared", "kubernetes.io/role/elb": "1", "KubernetesCluster": "${var.vpc_name}"})}'
- aws_route_table:
    eks_private:
      vpc_id: ${local.vpc_id}
      lifecycle:
      - {}
      tags:
        Name: eks_private
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
- aws_route:
    for_peering:
      route_table_id: ${aws_route_table.eks_private.id}
      destination_cidr_block: ${var.peering_cidr}
      vpc_peering_connection_id: ${data.aws_vpc_peering_connection.pc.id}
- aws_route:
    skip_proxy:
      count: ${length(var.cidrs_to_route_to_gw)}
      route_table_id: ${aws_route_table.eks_private.id}
      destination_cidr_block: ${element(var.cidrs_to_route_to_gw, count.index)}
      nat_gateway_id: ${data.aws_nat_gateway.the_gateway.id}
      depends_on:
      - ${aws_route_table.eks_private}
- aws_route:
    public_access:
      count: '${var.ha_squid ? var.dual_proxy ? 1 : 0 : 1}'
      destination_cidr_block: 0.0.0.0/0
      route_table_id: ${aws_route_table.eks_private.id}
      network_interface_id: ${data.aws_instances.squid_proxy[count.index].ids[0]}
- aws_route_table_association:
    private_kube:
      count: ${random_shuffle.az[0].result_count}
      subnet_id: ${aws_subnet.eks_private.*.id[count.index]}
      route_table_id: ${aws_route_table.eks_private.id}
      depends_on:
      - ${aws_subnet.eks_private}
- aws_route_table_association:
    secondary_subnet_kube:
      count: '${var.secondary_cidr_block != "" ? random_shuffle.secondary_az[0].result_count
        : 0}'
      subnet_id: ${aws_subnet.eks_secondary_subnet.*.id[count.index]}
      route_table_id: ${aws_route_table.eks_private.id}
      depends_on:
      - ${aws_subnet.eks_secondary_subnet}
- aws_security_group:
    eks_control_plane_sg:
      name: ${var.vpc_name}-control-plane
      description: Cluster communication with worker nodes [${var.vpc_name}]
      vpc_id: ${local.vpc_id}
      egress:
      - from_port: 0
        to_port: 0
        protocol: '-1'
        cidr_blocks:
        - 0.0.0.0/0
      tags:
        Name: ${var.vpc_name}-control-plane-sg
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
- aws_route_table_association:
    public_kube:
      count: ${random_shuffle.az[0].result_count}
      subnet_id: ${aws_subnet.eks_public.*.id[count.index]}
      route_table_id: ${data.aws_route_table.public_kube.id}
      lifecycle:
      - {}
- aws_eks_cluster:
    eks_cluster:
      name: ${var.vpc_name}
      role_arn: ${aws_iam_role.eks_control_plane_role.arn}
      version: ${var.eks_version}
      vpc_config:
      - subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
        security_group_ids:
        - ${aws_security_group.eks_control_plane_sg.id}
        endpoint_private_access: 'true'
        endpoint_public_access: ${var.eks_public_access}
      depends_on:
      - ${aws_iam_role_policy_attachment.eks-policy-AmazonEKSClusterPolicy}
      - ${aws_iam_role_policy_attachment.eks-policy-AmazonEKSServicePolicy}
      - ${aws_subnet.eks_private}
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
- aws_iam_role:
    eks_node_role:
      name: eks_${var.vpc_name}_workers_role
      assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n\
        \    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\"\
        : \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n\
        \    }\n  ]\n}"
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
- aws_iam_policy:
    cwl_access_policy:
      name: ${var.vpc_name}_EKS_workers_access_to_cloudwatchlogs
      description: In order to avoid the creation of users and keys, we are using
        roles and policies.
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n       \
        \ {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:DescribeLogGroups\"\
        ,\n            \"Resource\": \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group::log-stream:*\"\
        \n        },\n        {\n            \"Effect\": \"Allow\",\n            \"\
        Action\": [\n                \"logs:CreateLogStream\",\n                \"\
        logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\n      \
        \      ],\n            \"Resource\": \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group:${var.vpc_name}:log-stream:*\"\
        \n        }\n    ]\n}"
- aws_iam_policy:
    asg_access:
      name: ${var.vpc_name}_EKS_workers_autoscaling_access
      description: Allow the deployment cluster-autoscaler to add or terminate instances
        accordingly
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n       \
        \ {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n     \
        \           \"autoscaling:DescribeAutoScalingGroups\",\n                \"\
        autoscaling:DescribeAutoScalingInstances\",\n                \"autoscaling:DescribeTags\"\
        ,\n                \"autoscaling:SetDesiredCapacity\",\n                \"\
        autoscaling:TerminateInstanceInAutoScalingGroup\",\n                \"autoscaling:DescribeLaunchConfigurations\"\
        \n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}"
- aws_iam_role_policy:
    csoc_alert_sns_access:
      count: '${var.sns_topic_arn != "" ? 1 : 0}'
      name: ${var.vpc_name}_CSOC_alert_SNS_topic_acess
      policy: ${data.aws_iam_policy_document.planx-csoc-alerts-topic_access[count.index].json}
      role: ${aws_iam_role.eks_node_role.id}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEKSWorkerNodePolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEKS_CNI_Policy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEKSCSIDriverPolicy:
      policy_arn: arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEC2ContainerRegistryReadOnly:
      policy_arn: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    cloudwatch_logs_access:
      policy_arn: ${aws_iam_policy.cwl_access_policy.arn}
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    asg_access:
      policy_arn: ${aws_iam_policy.asg_access.arn}
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    bucket_read:
      policy_arn: arn:aws:iam::${data.aws_caller_identity.current.account_id}:policy/bucket_reader_cdis-gen3-users_${var.users_policy}
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-policy-AmazonSSMManagedInstanceCore:
      policy_arn: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_instance_profile:
    eks_node_instance_profile:
      name: ${var.vpc_name}_EKS_workers
      role: ${aws_iam_role.eks_node_role.name}
- aws_security_group:
    eks_nodes_sg:
      name: ${var.vpc_name}_EKS_workers_sg
      description: 'Security group for all nodes in the EKS cluster [${var.vpc_name}] '
      vpc_id: ${local.vpc_id}
      egress:
      - from_port: 0
        to_port: 0
        protocol: '-1'
        cidr_blocks:
        - 0.0.0.0/0
      tags: '${tomap({"Name": "${var.vpc_name}-nodes-sg", "kubernetes.io/cluster/${var.vpc_name}":
        "owned", "karpenter.sh/discovery": "${var.vpc_name}"})}'
- aws_security_group_rule:
    https_nodes_to_plane:
      type: ingress
      from_port: 443
      to_port: 443
      protocol: tcp
      security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      source_security_group_id: ${aws_security_group.eks_nodes_sg.id}
      depends_on:
      - ${aws_security_group.eks_nodes_sg}
      - ${aws_security_group.eks_control_plane_sg}
      description: from the workers to the control plane
- aws_security_group_rule:
    https_csoc_to_plane:
      count: '${var.csoc_managed ? 1 : 0}'
      type: ingress
      from_port: 443
      to_port: 443
      protocol: tcp
      security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      cidr_blocks:
      - ${var.peering_cidr}
      depends_on:
      - ${aws_security_group.eks_nodes_sg}
      - ${aws_security_group.eks_control_plane_sg}
      description: from the CSOC to the control plane
- aws_security_group_rule:
    communication_plane_to_nodes:
      type: ingress
      from_port: 80
      to_port: 65534
      protocol: tcp
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      depends_on:
      - ${aws_security_group.eks_nodes_sg}
      - ${aws_security_group.eks_control_plane_sg}
      description: from the control plane to the nodes
- aws_security_group_rule:
    nodes_internode_communications:
      type: ingress
      from_port: 0
      to_port: 0
      protocol: '-1'
      description: allow nodes to communicate with each other
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      self: true
- aws_security_group_rule:
    nodes_interpool_communications:
      count: '${var.deploy_jupyter ? 1 : 0}'
      type: ingress
      from_port: 0
      to_port: 0
      protocol: '-1'
      description: allow jupyter nodes to talk to the default
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${module.jupyter_pool[0].nodepool_sg}
- aws_security_group_rule:
    workflow_nodes_interpool_communications:
      count: '${var.deploy_workflow ? 1 : 0}'
      type: ingress
      from_port: 0
      to_port: 0
      protocol: '-1'
      description: allow workflow nodes to talk to the default
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${module.workflow_pool[0].nodepool_sg}
- aws_iam_service_linked_role:
    autoscaling:
      aws_service_name: autoscaling.amazonaws.com
      custom_suffix: ${var.vpc_name}
      lifecycle:
      - ignore_changes:
        - ${custom_suffix}
- aws_kms_grant:
    kms:
      count: '${var.fips ? 1 : 0}'
      name: kms-cmk-eks
      key_id: ${var.fips_ami_kms}
      grantee_principal: ${aws_iam_service_linked_role.autoscaling.arn}
      operations:
      - Encrypt
      - Decrypt
      - ReEncryptFrom
      - ReEncryptTo
      - GenerateDataKey
      - GenerateDataKeyWithoutPlaintext
      - DescribeKey
      - CreateGrant
- aws_security_group:
    ssh:
      name: ssh_eks_${var.vpc_name}
      description: security group that only enables ssh
      vpc_id: ${local.vpc_id}
      ingress:
      - from_port: 22
        to_port: 22
        protocol: TCP
        cidr_blocks:
        - 0.0.0.0/0
      tags: '${tomap({"Environment": "${var.vpc_name}", "Organization": "${var.organization_name}",
        "Name": "ssh_eks_${var.vpc_name}", "karpenter.sh/discovery": "${var.vpc_name}"})}'
- kubectl_manifest:
    aws-auth:
      count: '${var.k8s_bootstrap_resources ? 1 : 0}'
      yaml_body: ${local.config-map-aws-auth}
- null_resource:
    config_setup:
      triggers:
        kubeconfig_change: '${sensitive(templatefile("${path.module}/kubeconfig.tpl",
          {"vpc_name": "${var.vpc_name}", "eks_name": "${aws_eks_cluster.eks_cluster.id}",
          "eks_endpoint": "${aws_eks_cluster.eks_cluster.endpoint}", "eks_cert": "${aws_eks_cluster.eks_cluster.certificate_authority[0].data}"}))}'
        configmap_change: ${sensitive(local.config-map-aws-auth)}
      provisioner:
      - local-exec:
          command: mkdir -p ${var.vpc_name}_output_EKS; echo '${templatefile("${path.module}/kubeconfig.tpl",
            {vpc_name = var.vpc_name, eks_name = aws_eks_cluster.eks_cluster.id, eks_endpoint
            = aws_eks_cluster.eks_cluster.endpoint, eks_cert = aws_eks_cluster.eks_cluster.certificate_authority.0.data,})}'
            >${var.vpc_name}_output_EKS/kubeconfig
      - local-exec:
          command: echo "${local.config-map-aws-auth}" > ${var.vpc_name}_output_EKS/aws-auth-cm.yaml
      - local-exec:
          command: echo "${templatefile("${path.module}/init_cluster.sh", { vpc_name
            = var.vpc_name, kubeconfig_path = "${var.vpc_name}_output_EKS/kubeconfig",
            auth_configmap = "${var.vpc_name}_output_EKS/aws-auth-cm.yaml"})}" > ${var.vpc_name}_output_EKS/init_cluster.sh
      depends_on:
      - ${aws_autoscaling_group.eks_autoscaling_group}
- aws_iam_openid_connect_provider:
    identity_provider:
      count: '${var.iam-serviceaccount ? var.eks_version == "1.12" ? 0 : 1 : 0}'
      url: ${aws_eks_cluster.eks_cluster.identity.0.oidc.0.issuer}
      client_id_list:
      - sts.amazonaws.com
      thumbprint_list: ${var.oidc_eks_thumbprint}
      depends_on:
      - ${aws_eks_cluster.eks_cluster}
