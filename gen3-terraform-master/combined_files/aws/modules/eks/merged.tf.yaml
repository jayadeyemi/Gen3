resource:
- aws_launch_template:
    eks_launch_template:
      block_device_mappings:
      - device_name: /dev/xvda
        ebs:
        - volume_size: ${var.worker_drive_size}
      iam_instance_profile:
      - name: ${aws_iam_instance_profile.eks_node_instance_profile.name}
      image_id: ${local.ami}
      instance_type: ${var.instance_type}
      key_name: ${var.ec2_keyname}
      lifecycle:
      - create_before_destroy: true
      name_prefix: eks-${var.vpc_name}
      network_interfaces:
      - associate_public_ip_address: false
        security_groups:
        - ${aws_security_group.eks_nodes_sg.id}
        - ${aws_security_group.ssh.id}
      tag_specifications:
      - resource_type: instance
        tags:
          Name: eks-${var.vpc_name}-node
      user_data: '${sensitive(base64encode(templatefile("${path.module}/../../../../flavors/eks/${var.bootstrap_script}",
        {"eks_ca": "${aws_eks_cluster.eks_cluster.certificate_authority[0].data}",
        "eks_endpoint": "${aws_eks_cluster.eks_cluster.endpoint}", "eks_region": "${data.aws_region.current.name}",
        "vpc_name": "${var.vpc_name}", "ssh_keys": "${templatefile("${path.module}/../../../../files/authorized_keys/ops_team",
        {})}", "nodepool": "default", "lifecycle_type": "ONDEMAND", "activation_id":
        "${var.activation_id}", "customer_id": "${var.customer_id}"})))}'
- aws_autoscaling_group:
    eks_autoscaling_group:
      count: '${var.use_asg ? 1 : 0}'
      desired_capacity: 2
      launch_template:
      - id: ${aws_launch_template.eks_launch_template.id}
        version: $Latest
      lifecycle:
      - ignore_changes:
        - ${desired_capacity}
        - ${max_size}
        - ${min_size}
      max_size: 10
      min_size: 2
      name: eks-worker-node-${var.vpc_name}
      service_linked_role_arn: ${aws_iam_service_linked_role.autoscaling.arn}
      tag:
      - key: Environment
        propagate_at_launch: true
        value: ${var.vpc_name}
      - key: Name
        propagate_at_launch: true
        value: eks-${var.vpc_name}
      - key: kubernetes.io/cluster/${var.vpc_name}
        propagate_at_launch: true
        value: owned
      - key: k8s.io/cluster-autoscaler/enabled
        propagate_at_launch: true
        value: ''
      - key: k8s.io/cluster-type/eks
        propagate_at_launch: true
        value: ''
      - key: k8s.io/nodepool/default
        propagate_at_launch: true
        value: ''
      vpc_zone_identifier: ${flatten([${aws_subnet.eks_private.*.id}])}
---
locals:
- ami: '${var.fips ? var.fips_enabled_ami : data.aws_ami.eks_worker.id}'
  azs: '${var.availability_zones != 0 ? var.availability_zones : data.aws_availability_zones.available.names}'
  eks_priv_subnets: '${var.secondary_cidr_block != "" ? aws_subnet.eks_secondary_subnet.*.id
    : aws_subnet.eks_private.*.id}'
  secondary_azs: '${var.secondary_availability_zones != 0 ? var.secondary_availability_zones
    : data.aws_availability_zones.available.names}'
  vpc_id: '${var.vpc_id != "" ? var.vpc_id : data.aws_vpc.the_vpc.id}'
- config-map-aws-auth: "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-auth\n\
    \  namespace: kube-system\ndata:\n  mapRoles: |\n    - rolearn: ${aws_iam_role.eks_node_role.arn}\n\
    \      username: system:node:{{EC2PrivateDNSName}}\n      groups:\n        - system:bootstrappers\n\
    \        - system:nodes\n    - rolearn: ${aws_iam_role.karpenter[0].arn} \n  \
    \    username: system:node:{{SessionName}}\n      groups:\n        - system:bootstrappers\n\
    \        - system:nodes\n        - system:node-proxier\n    - rolearn: ${var.deploy_workflow\
    \ ? module.workflow_pool[0].nodepool_role : \"\"}\n      username: system:node:{{EC2PrivateDNSName}}\n\
    \      groups:\n        - system:bootstrappers\n        - system:nodes\n    -\
    \ rolearn: ${var.deploy_jupyter ? module.jupyter_pool[0].nodepool_role : \"\"\
    }\n      username: system:node:{{EC2PrivateDNSName}}\n      groups:\n        -\
    \ system:bootstrappers\n        - system:nodes"
module:
- jupyter_pool:
    activation_id: ${var.activation_id}
    bootstrap_script: ${var.jupyter_bootstrap_script}
    control_plane_sg: ${aws_security_group.eks_control_plane_sg.id}
    count: '${var.deploy_jupyter ? 1 : 0}'
    csoc_cidr: ${var.peering_cidr}
    customer_id: ${var.customer_id}
    default_nodepool_sg: ${aws_security_group.eks_nodes_sg.id}
    ec2_keyname: ${var.ec2_keyname}
    eks_cluster_ca: ${aws_eks_cluster.eks_cluster.certificate_authority[0].data}
    eks_cluster_endpoint: ${aws_eks_cluster.eks_cluster.endpoint}
    eks_private_subnets: ${aws_subnet.eks_private.*.id}
    eks_version: ${var.eks_version}
    kernel: ${var.kernel}
    nodepool: jupyter
    nodepool_asg_desired_capacity: ${var.jupyter_asg_desired_capacity}
    nodepool_asg_max_size: ${var.jupyter_asg_max_size}
    nodepool_asg_min_size: ${var.jupyter_asg_min_size}
    nodepool_instance_type: ${var.jupyter_instance_type}
    nodepool_worker_drive_size: ${var.jupyter_worker_drive_size}
    organization_name: ${var.organization_name}
    scale_in_protection: false
    source: ../eks-nodepool/
    users_policy: ${var.users_policy}
    vpc_id: ${local.vpc_id}
    vpc_name: ${var.vpc_name}
- workflow_pool:
    activation_id: ${var.activation_id}
    bootstrap_script: ${var.workflow_bootstrap_script}
    control_plane_sg: ${aws_security_group.eks_control_plane_sg.id}
    count: '${var.deploy_workflow ? 1 : 0}'
    csoc_cidr: ${var.peering_cidr}
    customer_id: ${var.customer_id}
    default_nodepool_sg: ${aws_security_group.eks_nodes_sg.id}
    ec2_keyname: ${var.ec2_keyname}
    eks_cluster_ca: ${aws_eks_cluster.eks_cluster.certificate_authority[0].data}
    eks_cluster_endpoint: ${aws_eks_cluster.eks_cluster.endpoint}
    eks_private_subnets: ${local.eks_priv_subnets}
    eks_version: ${var.eks_version}
    kernel: ${var.kernel}
    nodepool: workflow
    nodepool_asg_desired_capacity: ${var.workflow_asg_desired_capacity}
    nodepool_asg_max_size: ${var.workflow_asg_max_size}
    nodepool_asg_min_size: ${var.workflow_asg_min_size}
    nodepool_instance_type: ${var.workflow_instance_type}
    nodepool_worker_drive_size: ${var.workflow_worker_drive_size}
    organization_name: ${var.organization_name}
    scale_in_protection: true
    source: ../eks-nodepool/
    users_policy: ${var.users_policy}
    vpc_id: ${local.vpc_id}
    vpc_name: ${var.vpc_name}
resource:
- aws_iam_role:
    eks_control_plane_role:
      assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n\
        \    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n  \
        \      \"Service\": \"eks.amazonaws.com\"\n      },\n      \"Effect\": \"\
        Allow\",\n      \"Sid\": \"\"\n    }\n  ]\n}"
      name: ${var.vpc_name}_EKS_role
- aws_iam_role_policy_attachment:
    eks-policy-AmazonEKSClusterPolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
      role: ${aws_iam_role.eks_control_plane_role.name}
- aws_iam_role_policy_attachment:
    eks-policy-AmazonEKSServicePolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSServicePolicy
      role: ${aws_iam_role.eks_control_plane_role.name}
- random_shuffle:
    az:
      count: 1
      input: ${local.azs}
      result_count: ${length(local.azs)}
- random_shuffle:
    secondary_az:
      count: 1
      input: ${local.secondary_azs}
      result_count: ${length(local.secondary_azs)}
- aws_subnet:
    eks_private:
      availability_zone: ${random_shuffle.az[0].result[count.index]}
      cidr_block: '${var.workers_subnet_size == 22 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block,
        2, (1 + count.index)) : var.workers_subnet_size == 23 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block,
        3, (2 + count.index)) : cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 4, (7
        + count.index))}'
      count: ${random_shuffle.az[0].result_count}
      lifecycle:
      - ignore_changes:
        - ${tags}
        - ${availability_zone}
        - ${cidr_block}
      map_public_ip_on_launch: false
      tags: '${tomap({"Name": "eks_private_${count.index}", "Environment": "${var.vpc_name}",
        "Organization": "${var.organization_name}", "kubernetes.io/cluster/${var.vpc_name}":
        "owned", "kubernetes.io/role/internal-elb": "1", "karpenter.sh/discovery":
        "${var.vpc_name}"})}'
      vpc_id: ${local.vpc_id}
- aws_subnet:
    eks_secondary_subnet:
      availability_zone: ${random_shuffle.secondary_az[0].result[count.index]}
      cidr_block: ${cidrsubnet(var.secondary_cidr_block, 2, count.index)}
      count: '${var.secondary_cidr_block != "" ? 4 : 0}'
      lifecycle:
      - ignore_changes:
        - ${tags}
        - ${availability_zone}
        - ${cidr_block}
      map_public_ip_on_launch: false
      tags: '${tomap({"Name": "eks_secondary_cidr_subnet_${count.index}", "Environment":
        "${var.vpc_name}", "Organization": "${var.organization_name}", "kubernetes.io/cluster/${var.vpc_name}":
        "owned", "kubernetes.io/role/internal-elb": "1", "karpenter.sh/discovery":
        "${var.vpc_name}"})}'
      vpc_id: ${local.vpc_id}
- aws_subnet:
    eks_public:
      availability_zone: ${random_shuffle.az[0].result[count.index]}
      cidr_block: '${var.workers_subnet_size == 22 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block,
        5, (4 + count.index)) : cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 4, (10
        + count.index))}'
      count: ${random_shuffle.az[0].result_count}
      lifecycle:
      - ignore_changes:
        - ${tags}
        - ${availability_zone}
        - ${cidr_block}
      map_public_ip_on_launch: true
      tags: '${tomap({"Name": "eks_public_${count.index}", "Environment": "${var.vpc_name}",
        "Organization": "${var.organization_name}", "kubernetes.io/cluster/${var.vpc_name}":
        "shared", "kubernetes.io/role/elb": "1", "KubernetesCluster": "${var.vpc_name}"})}'
      vpc_id: ${local.vpc_id}
- aws_route_table:
    eks_private:
      lifecycle:
      - {}
      tags:
        Environment: ${var.vpc_name}
        Name: eks_private
        Organization: ${var.organization_name}
      vpc_id: ${local.vpc_id}
- aws_route:
    for_peering:
      destination_cidr_block: ${var.peering_cidr}
      route_table_id: ${aws_route_table.eks_private.id}
      vpc_peering_connection_id: ${data.aws_vpc_peering_connection.pc.id}
- aws_route:
    skip_proxy:
      count: ${length(var.cidrs_to_route_to_gw)}
      depends_on:
      - ${aws_route_table.eks_private}
      destination_cidr_block: ${element(var.cidrs_to_route_to_gw, count.index)}
      nat_gateway_id: ${data.aws_nat_gateway.the_gateway.id}
      route_table_id: ${aws_route_table.eks_private.id}
- aws_route:
    public_access:
      count: '${var.ha_squid ? var.dual_proxy ? 1 : 0 : 1}'
      destination_cidr_block: 0.0.0.0/0
      network_interface_id: ${data.aws_instances.squid_proxy[count.index].ids[0]}
      route_table_id: ${aws_route_table.eks_private.id}
- aws_route_table_association:
    private_kube:
      count: ${random_shuffle.az[0].result_count}
      depends_on:
      - ${aws_subnet.eks_private}
      route_table_id: ${aws_route_table.eks_private.id}
      subnet_id: ${aws_subnet.eks_private.*.id[count.index]}
- aws_route_table_association:
    secondary_subnet_kube:
      count: '${var.secondary_cidr_block != "" ? random_shuffle.secondary_az[0].result_count
        : 0}'
      depends_on:
      - ${aws_subnet.eks_secondary_subnet}
      route_table_id: ${aws_route_table.eks_private.id}
      subnet_id: ${aws_subnet.eks_secondary_subnet.*.id[count.index]}
- aws_security_group:
    eks_control_plane_sg:
      description: Cluster communication with worker nodes [${var.vpc_name}]
      egress:
      - cidr_blocks:
        - 0.0.0.0/0
        from_port: 0
        protocol: '-1'
        to_port: 0
      name: ${var.vpc_name}-control-plane
      tags:
        Environment: ${var.vpc_name}
        Name: ${var.vpc_name}-control-plane-sg
        Organization: ${var.organization_name}
      vpc_id: ${local.vpc_id}
- aws_route_table_association:
    public_kube:
      count: ${random_shuffle.az[0].result_count}
      lifecycle:
      - {}
      route_table_id: ${data.aws_route_table.public_kube.id}
      subnet_id: ${aws_subnet.eks_public.*.id[count.index]}
- aws_eks_cluster:
    eks_cluster:
      depends_on:
      - ${aws_iam_role_policy_attachment.eks-policy-AmazonEKSClusterPolicy}
      - ${aws_iam_role_policy_attachment.eks-policy-AmazonEKSServicePolicy}
      - ${aws_subnet.eks_private}
      name: ${var.vpc_name}
      role_arn: ${aws_iam_role.eks_control_plane_role.arn}
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      version: ${var.eks_version}
      vpc_config:
      - endpoint_private_access: 'true'
        endpoint_public_access: ${var.eks_public_access}
        security_group_ids:
        - ${aws_security_group.eks_control_plane_sg.id}
        subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
- aws_iam_role:
    eks_node_role:
      assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n\
        \    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\"\
        : \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n\
        \    }\n  ]\n}"
      name: eks_${var.vpc_name}_workers_role
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
- aws_iam_policy:
    cwl_access_policy:
      description: In order to avoid the creation of users and keys, we are using
        roles and policies.
      name: ${var.vpc_name}_EKS_workers_access_to_cloudwatchlogs
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n       \
        \ {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:DescribeLogGroups\"\
        ,\n            \"Resource\": \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group::log-stream:*\"\
        \n        },\n        {\n            \"Effect\": \"Allow\",\n            \"\
        Action\": [\n                \"logs:CreateLogStream\",\n                \"\
        logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\n      \
        \      ],\n            \"Resource\": \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group:${var.vpc_name}:log-stream:*\"\
        \n        }\n    ]\n}"
- aws_iam_policy:
    asg_access:
      description: Allow the deployment cluster-autoscaler to add or terminate instances
        accordingly
      name: ${var.vpc_name}_EKS_workers_autoscaling_access
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n       \
        \ {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n     \
        \           \"autoscaling:DescribeAutoScalingGroups\",\n                \"\
        autoscaling:DescribeAutoScalingInstances\",\n                \"autoscaling:DescribeTags\"\
        ,\n                \"autoscaling:SetDesiredCapacity\",\n                \"\
        autoscaling:TerminateInstanceInAutoScalingGroup\",\n                \"autoscaling:DescribeLaunchConfigurations\"\
        \n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}"
- aws_iam_role_policy:
    csoc_alert_sns_access:
      count: '${var.sns_topic_arn != "" ? 1 : 0}'
      name: ${var.vpc_name}_CSOC_alert_SNS_topic_acess
      policy: ${data.aws_iam_policy_document.planx-csoc-alerts-topic_access[count.index].json}
      role: ${aws_iam_role.eks_node_role.id}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEKSWorkerNodePolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEKS_CNI_Policy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEKSCSIDriverPolicy:
      policy_arn: arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEC2ContainerRegistryReadOnly:
      policy_arn: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    cloudwatch_logs_access:
      policy_arn: ${aws_iam_policy.cwl_access_policy.arn}
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    asg_access:
      policy_arn: ${aws_iam_policy.asg_access.arn}
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    bucket_read:
      policy_arn: arn:aws:iam::${data.aws_caller_identity.current.account_id}:policy/bucket_reader_cdis-gen3-users_${var.users_policy}
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-policy-AmazonSSMManagedInstanceCore:
      policy_arn: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_instance_profile:
    eks_node_instance_profile:
      name: ${var.vpc_name}_EKS_workers
      role: ${aws_iam_role.eks_node_role.name}
- aws_security_group:
    eks_nodes_sg:
      description: 'Security group for all nodes in the EKS cluster [${var.vpc_name}] '
      egress:
      - cidr_blocks:
        - 0.0.0.0/0
        from_port: 0
        protocol: '-1'
        to_port: 0
      name: ${var.vpc_name}_EKS_workers_sg
      tags: '${tomap({"Name": "${var.vpc_name}-nodes-sg", "kubernetes.io/cluster/${var.vpc_name}":
        "owned", "karpenter.sh/discovery": "${var.vpc_name}"})}'
      vpc_id: ${local.vpc_id}
- aws_security_group_rule:
    https_nodes_to_plane:
      depends_on:
      - ${aws_security_group.eks_nodes_sg}
      - ${aws_security_group.eks_control_plane_sg}
      description: from the workers to the control plane
      from_port: 443
      protocol: tcp
      security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      source_security_group_id: ${aws_security_group.eks_nodes_sg.id}
      to_port: 443
      type: ingress
- aws_security_group_rule:
    https_csoc_to_plane:
      cidr_blocks:
      - ${var.peering_cidr}
      count: '${var.csoc_managed ? 1 : 0}'
      depends_on:
      - ${aws_security_group.eks_nodes_sg}
      - ${aws_security_group.eks_control_plane_sg}
      description: from the CSOC to the control plane
      from_port: 443
      protocol: tcp
      security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      to_port: 443
      type: ingress
- aws_security_group_rule:
    communication_plane_to_nodes:
      depends_on:
      - ${aws_security_group.eks_nodes_sg}
      - ${aws_security_group.eks_control_plane_sg}
      description: from the control plane to the nodes
      from_port: 80
      protocol: tcp
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      to_port: 65534
      type: ingress
- aws_security_group_rule:
    nodes_internode_communications:
      description: allow nodes to communicate with each other
      from_port: 0
      protocol: '-1'
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      self: true
      to_port: 0
      type: ingress
- aws_security_group_rule:
    nodes_interpool_communications:
      count: '${var.deploy_jupyter ? 1 : 0}'
      description: allow jupyter nodes to talk to the default
      from_port: 0
      protocol: '-1'
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${module.jupyter_pool[0].nodepool_sg}
      to_port: 0
      type: ingress
- aws_security_group_rule:
    workflow_nodes_interpool_communications:
      count: '${var.deploy_workflow ? 1 : 0}'
      description: allow workflow nodes to talk to the default
      from_port: 0
      protocol: '-1'
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${module.workflow_pool[0].nodepool_sg}
      to_port: 0
      type: ingress
- aws_iam_service_linked_role:
    autoscaling:
      aws_service_name: autoscaling.amazonaws.com
      custom_suffix: ${var.vpc_name}
      lifecycle:
      - ignore_changes:
        - ${custom_suffix}
- aws_kms_grant:
    kms:
      count: '${var.fips ? 1 : 0}'
      grantee_principal: ${aws_iam_service_linked_role.autoscaling.arn}
      key_id: ${var.fips_ami_kms}
      name: kms-cmk-eks
      operations:
      - Encrypt
      - Decrypt
      - ReEncryptFrom
      - ReEncryptTo
      - GenerateDataKey
      - GenerateDataKeyWithoutPlaintext
      - DescribeKey
      - CreateGrant
- aws_security_group:
    ssh:
      description: security group that only enables ssh
      ingress:
      - cidr_blocks:
        - 0.0.0.0/0
        from_port: 22
        protocol: TCP
        to_port: 22
      name: ssh_eks_${var.vpc_name}
      tags: '${tomap({"Environment": "${var.vpc_name}", "Organization": "${var.organization_name}",
        "Name": "ssh_eks_${var.vpc_name}", "karpenter.sh/discovery": "${var.vpc_name}"})}'
      vpc_id: ${local.vpc_id}
- kubectl_manifest:
    aws-auth:
      count: '${var.k8s_bootstrap_resources ? 1 : 0}'
      yaml_body: ${local.config-map-aws-auth}
- null_resource:
    config_setup:
      depends_on:
      - ${aws_autoscaling_group.eks_autoscaling_group}
      provisioner:
      - local-exec:
          command: mkdir -p ${var.vpc_name}_output_EKS; echo '${templatefile("${path.module}/kubeconfig.tpl",
            {vpc_name = var.vpc_name, eks_name = aws_eks_cluster.eks_cluster.id, eks_endpoint
            = aws_eks_cluster.eks_cluster.endpoint, eks_cert = aws_eks_cluster.eks_cluster.certificate_authority.0.data,})}'
            >${var.vpc_name}_output_EKS/kubeconfig
      - local-exec:
          command: echo "${local.config-map-aws-auth}" > ${var.vpc_name}_output_EKS/aws-auth-cm.yaml
      - local-exec:
          command: echo "${templatefile("${path.module}/init_cluster.sh", { vpc_name
            = var.vpc_name, kubeconfig_path = "${var.vpc_name}_output_EKS/kubeconfig",
            auth_configmap = "${var.vpc_name}_output_EKS/aws-auth-cm.yaml"})}" > ${var.vpc_name}_output_EKS/init_cluster.sh
      triggers:
        configmap_change: ${sensitive(local.config-map-aws-auth)}
        kubeconfig_change: '${sensitive(templatefile("${path.module}/kubeconfig.tpl",
          {"vpc_name": "${var.vpc_name}", "eks_name": "${aws_eks_cluster.eks_cluster.id}",
          "eks_endpoint": "${aws_eks_cluster.eks_cluster.endpoint}", "eks_cert": "${aws_eks_cluster.eks_cluster.certificate_authority[0].data}"}))}'
- aws_iam_openid_connect_provider:
    identity_provider:
      client_id_list:
      - sts.amazonaws.com
      count: '${var.iam-serviceaccount ? var.eks_version == "1.12" ? 0 : 1 : 0}'
      depends_on:
      - ${aws_eks_cluster.eks_cluster}
      thumbprint_list: ${var.oidc_eks_thumbprint}
      url: ${aws_eks_cluster.eks_cluster.identity.0.oidc.0.issuer}
terraform:
- required_providers:
  - kubectl:
      source: gavinbunney/kubectl
---
data:
- aws_caller_identity:
    current: {}
- aws_region:
    current: {}
- aws_vpc:
    the_vpc:
      id: ${data.aws_vpcs.vpcs.ids[0]}
- aws_availability_zones:
    available:
      state: available
- aws_vpcs:
    vpcs:
      tags:
        Name: ${var.vpc_name}
- aws_nat_gateway:
    the_gateway:
      state: available
      tags:
        Name: ${var.vpc_name}-ngw
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_vpc_peering_connection:
    pc:
      peer_owner_id: ${var.csoc_account_id}
      status: active
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_vpc_endpoint_service:
    logs:
      service: logs
- aws_vpc_endpoint_service:
    ec2:
      service: ec2
- aws_vpc_endpoint_service:
    autoscaling:
      service: autoscaling
- aws_vpc_endpoint_service:
    ecr_dkr:
      service: ecr.dkr
- aws_vpc_endpoint_service:
    ecr_api:
      service: ecr.api
- aws_vpc_endpoint_service:
    ebs:
      service: ebs
- aws_vpc_endpoint_service:
    sts:
      service: sts
- aws_route_table:
    public_kube:
      tags:
        Name: main
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_ami:
    eks_worker:
      filter:
      - name: name
        values:
        - amazon-eks-node-${var.eks_version}*
      most_recent: true
      owners:
      - '602401143452'
- aws_security_group:
    local_traffic:
      name: local
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_autoscaling_group:
    squid_auto:
      count: '${var.ha_squid ? 1 : 0}'
      name: squid-auto-${var.vpc_name}
- aws_instances:
    squid_proxy:
      count: '${var.ha_squid ? var.dual_proxy ? 1 : 0 : 1}'
      instance_tags:
        Name: ${var.vpc_name}${var.proxy_name}
- aws_route_table:
    private_kube_route_table:
      tags:
        Name: private_kube
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_route53_zone:
    vpczone:
      name: internal.io.
      vpc_id: ${data.aws_vpc.the_vpc.id}
- archive_file:
    lambda_function:
      output_path: lambda_function_payload.zip
      source_file: ${path.module}/lambda_function.py
      type: zip
- aws_iam_policy_document:
    with_resources:
      statement:
      - actions:
        - ec2:CreateRoute
        - ec2:DeleteRoute
        - ec2:ReplaceRoute
        - route53:GetHostedZone
        - route53:ChangeResourceRecordSets
        - route53:ListResourceRecordSets
        effect: Allow
        resources:
        - arn:aws:ec2:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:route-table/${aws_route_table.eks_private.id}
        - arn:aws:ec2:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:route-table/${data.aws_route_table.private_kube_route_table.id}
        - arn:aws:route53:::hostedzone/${data.aws_route53_zone.vpczone.zone_id}
- aws_iam_policy_document:
    without_resources:
      statement:
      - actions:
        - autoscaling:DescribeAutoScalingInstances
        - route53:CreateHostedZone
        - ec2:DescribeInstances
        - route53:ListHostedZones
        - ec2:DeleteNetworkInterface
        - ec2:DisassociateRouteTable
        - ec2:DescribeSecurityGroups
        - ec2:AssociateRouteTable
        - ec2:CreateNetworkInterface
        - ec2:DescribeNetworkInterfaces
        - autoscaling:DescribeAutoScalingGroups
        - ec2:DescribeVpcs
        - ec2:DescribeSubnets
        - ec2:DescribeRouteTables
        - ec2:DescribeInstanceAttribute
        - ec2:ModifyInstanceAttribute
        effect: Allow
        resources:
        - '*'
- aws_iam_policy_document:
    planx-csoc-alerts-topic_access:
      count: '${var.sns_topic_arn != "" ? 1 : 0}'
      statement:
      - actions:
        - sns:Publish
        effect: Allow
        resources:
        - ${var.sns_topic_arn}
- aws_ecrpublic_authorization_token:
    token: {}
---
resource:
- aws_vpc_endpoint:
    ec2:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      lifecycle:
      - ignore_changes: ${all}
      private_dns_enabled: true
      security_group_ids:
      - ${data.aws_security_group.local_traffic.id}
      service_name: ${data.aws_vpc_endpoint_service.ec2.service_name}
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Environment: ${var.vpc_name}
        Name: to ec2
        Organization: ${var.organization_name}
      vpc_endpoint_type: Interface
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_vpc_endpoint:
    sts:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      lifecycle:
      - ignore_changes: ${all}
      private_dns_enabled: true
      security_group_ids:
      - ${data.aws_security_group.local_traffic.id}
      service_name: ${data.aws_vpc_endpoint_service.sts.service_name}
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Environment: ${var.vpc_name}
        Name: to sts
        Organization: ${var.organization_name}
      vpc_endpoint_type: Interface
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_vpc_endpoint:
    autoscaling:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      lifecycle:
      - ignore_changes: ${all}
      private_dns_enabled: true
      security_group_ids:
      - ${data.aws_security_group.local_traffic.id}
      service_name: ${data.aws_vpc_endpoint_service.autoscaling.service_name}
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Environment: ${var.vpc_name}
        Name: to autoscaling
        Organization: ${var.organization_name}
      vpc_endpoint_type: Interface
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_vpc_endpoint:
    ecr-dkr:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      lifecycle:
      - ignore_changes: ${all}
      private_dns_enabled: true
      security_group_ids:
      - ${data.aws_security_group.local_traffic.id}
      service_name: ${data.aws_vpc_endpoint_service.ecr_dkr.service_name}
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Environment: ${var.vpc_name}
        Name: to ecr dkr
        Organization: ${var.organization_name}
      vpc_endpoint_type: Interface
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_vpc_endpoint:
    ecr-api:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      lifecycle:
      - ignore_changes: ${all}
      private_dns_enabled: true
      security_group_ids:
      - ${data.aws_security_group.local_traffic.id}
      service_name: ${data.aws_vpc_endpoint_service.ecr_api.service_name}
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Environment: ${var.vpc_name}
        Name: to ecr api
        Organization: ${var.organization_name}
      vpc_endpoint_type: Interface
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_vpc_endpoint:
    ebs:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      lifecycle:
      - ignore_changes: ${all}
      private_dns_enabled: true
      security_group_ids:
      - ${data.aws_security_group.local_traffic.id}
      service_name: ${data.aws_vpc_endpoint_service.ebs.service_name}
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Environment: ${var.vpc_name}
        Name: to ebs
        Organization: ${var.organization_name}
      vpc_endpoint_type: Interface
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_vpc_endpoint:
    k8s-s3:
      depends_on:
      - ${aws_route_table.eks_private}
      lifecycle:
      - ignore_changes: ${all}
      route_table_ids: ${flatten([${data.aws_route_table.public_kube.id}, ${aws_route_table.eks_private[*].id}])}
      service_name: com.amazonaws.${data.aws_region.current.name}.s3
      tags:
        Environment: ${var.vpc_name}
        Name: to s3
        Organization: ${var.organization_name}
      vpc_id: ${data.aws_vpc.the_vpc.id}
- aws_vpc_endpoint:
    k8s-logs:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      lifecycle:
      - ignore_changes: ${all}
      private_dns_enabled: true
      security_group_ids:
      - ${data.aws_security_group.local_traffic.id}
      service_name: ${data.aws_vpc_endpoint_service.logs.service_name}
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Environment: ${var.vpc_name}
        Name: to cloudwatch logs
        Organization: ${var.organization_name}
      vpc_endpoint_type: Interface
      vpc_id: ${data.aws_vpc.the_vpc.id}
---
data:
- aws_iam_policy_document:
    irsa_assume_role:
      count: '${var.use_karpenter ? 1 : 0}'
      statement:
      - actions:
        - sts:AssumeRoleWithWebIdentity
        condition:
        - test: StringEquals
          values:
          - system:serviceaccount:karpenter:karpenter
          variable: ${local.irsa_oidc_provider_url}:sub
        - test: StringEquals
          values:
          - sts.amazonaws.com
          variable: ${local.irsa_oidc_provider_url}:aud
        effect: Allow
        principals:
        - identifiers:
          - arn:aws:iam::${local.account_id}:oidc-provider/${local.irsa_oidc_provider_url}
          type: Federated
- aws_iam_policy_document:
    irsa:
      count: '${var.use_karpenter ? 1 : 0}'
      statement:
      - actions:
        - ssm:GetParameter
        - iam:PassRole
        - iam:*InstanceProfile
        - ec2:DescribeImages
        - ec2:RunInstances
        - ec2:DescribeSubnets
        - ec2:DescribeSecurityGroups
        - ec2:DescribeLaunchTemplates
        - ec2:DescribeInstances
        - ec2:DescribeInstanceTypes
        - ec2:DescribeInstanceTypeOfferings
        - ec2:DescribeAvailabilityZones
        - ec2:DeleteLaunchTemplate
        - ec2:CreateTags
        - ec2:CreateLaunchTemplate
        - ec2:CreateFleet
        - ec2:DescribeSpotPriceHistory
        - pricing:GetProducts
        - eks:DescribeCluster
        effect: Allow
        resources:
        - '*'
        sid: Karpenter
      - actions:
        - sqs:DeleteMessage
        - sqs:GetQueueAttributes
        - sqs:GetQueueUrl
        - sqs:ReceiveMessage
        effect: Allow
        resources:
        - arn:aws:sqs:*:${local.account_id}:karpenter-sqs-vpc_name
        sid: Karpenter2
      - actions:
        - ec2:TerminateInstances
        condition:
        - test: StringLike
          values:
          - '*karpenter*'
          variable: ec2:ResourceTag/Name
        effect: Allow
        resources:
        - '*'
        sid: ConditionalEC2Termination
      - actions:
        - kms:*
        effect: Allow
        resources:
        - '*'
        sid: VisualEditor0
- aws_iam_policy_document:
    queue:
      count: '${var.use_karpenter ? 1 : 0}'
      statement:
      - actions:
        - sqs:SendMessage
        principals:
        - identifiers:
          - events.amazonaws.com
          - sqs.amazonaws.com
          type: Service
        resources:
        - ${aws_sqs_queue.this[0].arn}
        sid: SqsWrite
locals:
- account_id: ${data.aws_caller_identity.current.account_id}
- create_irsa: true
  irsa_name: ${var.vpc_name}-karpenter-sa
  irsa_oidc_provider_url: ${replace(aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer,
    "https://", "")}
  irsa_policy_name: ${local.irsa_name}
- irsa_tag_values:
  - ${aws_eks_cluster.eks_cluster.id}
- enable_spot_termination: true
  queue_name: ${var.vpc_name}-${aws_eks_cluster.eks_cluster.id}
- events:
    health_event:
      description: Karpenter interrupt - AWS health event
      event_pattern:
        detail-type:
        - AWS Health Event
        source:
        - aws.health
      name: HealthEvent
    instance_rebalance:
      description: Karpenter interrupt - EC2 instance rebalance recommendation
      event_pattern:
        detail-type:
        - EC2 Instance Rebalance Recommendation
        source:
        - aws.ec2
      name: InstanceRebalance
    instance_state_change:
      description: Karpenter interrupt - EC2 instance state-change notification
      event_pattern:
        detail-type:
        - EC2 Instance State-change Notification
        source:
        - aws.ec2
      name: InstanceStateChange
    spot_interupt:
      description: Karpenter interrupt - EC2 spot instance interruption warning
      event_pattern:
        detail-type:
        - EC2 Spot Instance Interruption Warning
        source:
        - aws.ec2
      name: SpotInterrupt
resource:
- aws_iam_role:
    irsa:
      assume_role_policy: ${data.aws_iam_policy_document.irsa_assume_role[0].json}
      count: '${var.use_karpenter ? 1 : 0}'
      force_detach_policies: true
      name: ${local.irsa_name}
- aws_iam_service_linked_role:
    ec2_spot:
      aws_service_name: spot.amazonaws.com
      count: '${var.use_karpenter && var.spot_linked_role ? 1 : 0}'
      description: Service-linked role for EC2 Spot Instances
- aws_iam_policy:
    irsa:
      count: '${var.use_karpenter ? 1 : 0}'
      name: ${local.irsa_policy_name}-policy
      policy: ${data.aws_iam_policy_document.irsa[0].json}
- aws_iam_role_policy_attachment:
    irsa:
      count: '${var.use_karpenter ? 1 : 0}'
      policy_arn: ${aws_iam_policy.irsa[0].arn}
      role: ${aws_iam_role.irsa[0].name}
- aws_sqs_queue:
    this:
      count: '${var.use_karpenter ? 1 : 0}'
      message_retention_seconds: 300
      name: ${local.queue_name}
- aws_sqs_queue_policy:
    this:
      count: '${var.use_karpenter ? 1 : 0}'
      policy: ${data.aws_iam_policy_document.queue[0].json}
      queue_url: ${aws_sqs_queue.this[0].url}
- aws_cloudwatch_event_rule:
    this:
      description: ${each.value.description}
      event_pattern: ${jsonencode(each.value.event_pattern)}
      for_each: '${{for k , v in local.events : k => v if var.use_karpenter}}'
      name_prefix: ${var.vpc_name}-${each.value.name}-
      tags:
        ClusterName: ${aws_eks_cluster.eks_cluster.id}
- aws_cloudwatch_event_target:
    this:
      arn: ${aws_sqs_queue.this[0].arn}
      for_each: '${{for k , v in local.events : k => v if var.use_karpenter}}'
      rule: ${aws_cloudwatch_event_rule.this[each.key].name}
      target_id: KarpenterInterruptionQueueTarget
- aws_eks_fargate_profile:
    karpenter:
      cluster_name: ${aws_eks_cluster.eks_cluster.name}
      count: '${var.use_karpenter ? 1 : 0}'
      fargate_profile_name: karpenter
      pod_execution_role_arn: ${aws_iam_role.karpenter[0].arn}
      selector:
      - namespace: karpenter
      subnet_ids: ${local.eks_priv_subnets}
- time_sleep:
    wait_60_seconds:
      create_duration: 60s
      depends_on:
      - ${aws_eks_fargate_profile.karpenter}
- aws_iam_role:
    karpenter:
      assume_role_policy: '${jsonencode({"Statement": [{"Action": "sts:AssumeRole",
        "Effect": "Allow", "Principal": {"Service": "eks-fargate-pods.amazonaws.com"}}],
        "Version": "2012-10-17"})}'
      count: '${var.use_karpenter ? 1 : 0}'
      name: ${var.vpc_name}-karpenter-fargate-role
- aws_iam_role_policy_attachment:
    karpenter-role-policy:
      count: '${var.use_karpenter ? 1 : 0}'
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy
      role: ${aws_iam_role.karpenter[0].name}
- helm_release:
    karpenter:
      chart: karpenter
      count: '${var.k8s_bootstrap_resources && (var.use_karpenter || var.deploy_karpenter_in_k8s)
        ? 1 : 0}'
      create_namespace: true
      depends_on:
      - ${time_sleep.wait_60_seconds}
      name: karpenter
      namespace: karpenter
      repository: oci://public.ecr.aws/karpenter
      set:
      - name: settings.aws.clusterName
        value: ${aws_eks_cluster.eks_cluster.id}
      - name: settings.aws.clusterEndpoint
        value: ${aws_eks_cluster.eks_cluster.endpoint}
      - name: serviceAccount.annotations.eks\.amazonaws\.com/role-arn
        value: ${aws_iam_role.irsa[0].arn}
      - name: settings.aws.defaultInstanceProfile
        value: ${aws_iam_instance_profile.eks_node_instance_profile.id}
      - name: settings.aws.interruptionQueueName
        value: ${aws_sqs_queue.this[0].name}
      - name: dnsPolicy
        value: Default
      version: ${var.karpenter_version}
- kubectl_manifest:
    karpenter_node_pool:
      count: '${var.k8s_bootstrap_resources && (var.use_karpenter || var.deploy_karpenter_in_k8s)
        ? 1 : 0}'
      depends_on:
      - ${helm_release.karpenter}
      yaml_body: "---\napiVersion: karpenter.sh/v1beta1\nkind: NodePool\nmetadata:\n\
        \  name: default\nspec:\n  disruption:\n    consolidateAfter: 30s\n    consolidationPolicy:\
        \ WhenEmpty\n    expireAfter: \"168h\"\n  limits:\n    cpu: \"1000\"\n   \
        \ memory: 1000Gi\n  template:\n    metadata:\n      labels:\n        role:\
        \ default\n    spec:\n      kubelet:\n        evictionHard:\n          memory.available:\
        \ 5%\n        evictionSoft:\n          memory.available: 10%\n        evictionSoftGracePeriod:\n\
        \          memory.available: 5m\n        kubeReserved:\n          cpu: 480m\n\
        \          ephemeral-storage: 3Gi\n          memory: 1632Mi\n      nodeClassRef:\n\
        \        apiVersion: karpenter.k8s.aws/v1beta1\n        kind: EC2NodeClass\n\
        \        name: default\n      requirements:\n      - key: karpenter.sh/capacity-type\n\
        \        operator: In\n        values:\n        - on-demand\n        - spot\n\
        \      - key: kubernetes.io/arch\n        operator: In\n        values:\n\
        \        - amd64\n      - key: karpenter.k8s.aws/instance-category\n     \
        \   operator: In\n        values:\n        - c\n        - m\n        - r\n\
        \        - t"
- kubectl_manifest:
    karpenter_node_class:
      count: '${var.k8s_bootstrap_resources && (var.use_karpenter || var.deploy_karpenter_in_k8s)
        ? 1 : 0}'
      depends_on:
      - ${helm_release.karpenter}
      yaml_body: "    ---\n    apiVersion: karpenter.k8s.aws/v1beta1\n    kind: EC2NodeClass\n\
        \    metadata:\n      name: default\n    spec:\n      amiFamily: AL2\n   \
        \   amiSelectorTerms:\n      - name: \"EKS-FIPS*\"\n        owner: \"143731057154\"\
        \n      blockDeviceMappings:\n      - deviceName: /dev/xvda\n        ebs:\n\
        \          deleteOnTermination: true\n          encrypted: true\n        \
        \  volumeSize: ${var.worker_drive_size}Gi\n          volumeType: gp3\n   \
        \   metadataOptions:\n        httpEndpoint: enabled\n        httpProtocolIPv6:\
        \ disabled\n        httpPutResponseHopLimit: 2\n        httpTokens: optional\n\
        \      role: eks_${var.vpc_name}_workers_role\n\n      securityGroupSelectorTerms:\n\
        \      - tags:\n          karpenter.sh/discovery: ${var.vpc_name}\n\n    \
        \  subnetSelectorTerms:\n      - tags:\n          karpenter.sh/discovery:\
        \ ${var.vpc_name}\n\n      tags:\n        Environment: ${var.vpc_name}\n \
        \       Name: eks-${var.vpc_name}-karpenter\n        karpenter.sh/discovery:\
        \ ${var.vpc_name}\n        purpose: default\n\n      userData: |\n       \
        \ MIME-Version: 1.0\n        Content-Type: multipart/mixed; boundary=\"BOUNDARY\"\
        \n\n        --BOUNDARY\n        Content-Type: text/x-shellscript; charset=\"\
        us-ascii\"\n\n        #!/bin/bash -x\n        instanceId=$(curl -s http://169.254.169.254/latest/dynamic/instance-identity/document\
        \ | jq -r .instanceId)\n        curl https://raw.githubusercontent.com/uc-cdis/cloud-automation/master/files/authorized_keys/ops_team\
        \ >> /home/ec2-user/.ssh/authorized_keys\n        echo \"$(jq '.registryPullQPS=0'\
        \ /etc/kubernetes/kubelet/kubelet-config.json)\" > /etc/kubernetes/kubelet/kubelet-config.json\n\
        \        sysctl -w fs.inotify.max_user_watches=12000\n\n        sudo yum update\
        \ -y\n\n        --BOUNDARY--"
---
module:
- iam_role:
    count: '${var.ha_squid ? 1 : 0}'
    role_assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\":\
      \ [\n    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n\
      \        \"Service\": \"lambda.amazonaws.com\"\n      },\n      \"Effect\":\
      \ \"Allow\",\n      \"Sid\": \"\"\n    }\n  ]\n}"
    role_description: Role for ${var.vpc_name}-gw-checks-lambda
    role_name: ${var.vpc_name}-gw-checks-lambda-role
    role_tags:
      Environment: ${var.vpc_name}
      Organization: ${var.organization_name}
    source: ../iam-role
- iam_policy:
    count: '${var.ha_squid ? 1 : 0}'
    policy_description: IAM policy for ${var.vpc_name}-gw-checks-lambda to access
      CWLG
    policy_json: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n \
      \     \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n  \
      \      \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n       \
      \ \"logs:PutLogEvents\"\n      ],\n      \"Resource\": \"arn:aws:logs:*:*:*\"\
      \n    }\n  ]\n}"
    policy_name: ${var.vpc_name}-gw-checks-lambda-cwlg
    policy_path: /
    source: ../iam-policy
resource:
- aws_cloudwatch_log_group:
    gwl_group:
      count: '${var.ha_squid ? 1 : 0}'
      name: /aws/lambda/${var.vpc_name}-gw-checks-lambda
      retention_in_days: 14
- aws_iam_role_policy:
    lambda_policy_resources:
      count: '${var.ha_squid ? 1 : 0}'
      name: resources_acces
      policy: ${data.aws_iam_policy_document.with_resources.json}
      role: ${module.iam_role[0].role_id}
- aws_iam_role_policy:
    lambda_policy_no_resources:
      count: '${var.ha_squid ? 1 : 0}'
      name: no_resources_acces
      policy: ${data.aws_iam_policy_document.without_resources.json}
      role: ${module.iam_role[0].role_id}
- aws_iam_role_policy_attachment:
    lambda_logs:
      count: '${var.ha_squid ? 1 : 0}'
      policy_arn: ${module.iam_policy[0].arn}
      role: ${module.iam_role[0].role_id}
- aws_lambda_function:
    gw_checks:
      count: '${var.ha_squid ? 1 : 0}'
      depends_on:
      - ${aws_cloudwatch_log_group.gwl_group}
      description: Checks for internet access from the worker nodes subnets
      environment:
      - variables:
          domain_test: ${var.domain_test}
          vpc_name: ${var.vpc_name}
      filename: lambda_function_payload.zip
      function_name: ${var.vpc_name}-gw-checks-lambda
      handler: lambda_function.lambda_handler
      role: ${module.iam_role[0].role_arn}
      runtime: python3.8
      source_code_hash: ${data.archive_file.lambda_function.output_base64sha256}
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      timeout: 45
      vpc_config:
      - security_group_ids:
        - ${aws_security_group.eks_nodes_sg.id}
        subnet_ids: ${flatten([${aws_subnet.eks_private.*.id}])}
- aws_cloudwatch_event_rule:
    gw_checks_rule:
      count: '${var.ha_squid ? 1 : 0}'
      description: Check if the gateway is working every minute
      name: ${var.vpc_name}-GW-checks-job
      schedule_expression: rate(1 minute)
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
- aws_cloudwatch_event_target:
    cw_to_lambda:
      arn: ${aws_lambda_function.gw_checks[count.index].arn}
      count: '${var.ha_squid ? 1 : 0}'
      rule: ${aws_cloudwatch_event_rule.gw_checks_rule[count.index].name}
- aws_lambda_permission:
    allow_cloudwatch:
      action: lambda:InvokeFunction
      count: '${var.ha_squid ? 1 : 0}'
      function_name: ${aws_lambda_function.gw_checks[count.index].function_name}
      principal: events.amazonaws.com
      source_arn: ${aws_cloudwatch_event_rule.gw_checks_rule[count.index].arn}
      statement_id: AllowExecutionFromCloudWatch
---
output:
- kubeconfig:
    sensitive: true
    value: '${templatefile("${path.module}/kubeconfig.tpl", {"vpc_name": "${var.vpc_name}",
      "eks_name": "${aws_eks_cluster.eks_cluster.id}", "eks_endpoint": "${aws_eks_cluster.eks_cluster.endpoint}",
      "eks_cert": "${aws_eks_cluster.eks_cluster.certificate_authority[0].data}"})}'
- config_map_aws_auth:
    sensitive: true
    value: ${local.config-map-aws-auth}
- cluster_endpoint:
    sensitive: true
    value: ${aws_eks_cluster.eks_cluster.endpoint}
- cluster_certificate_authority_data:
    sensitive: true
    value: ${aws_eks_cluster.eks_cluster.certificate_authority[0].data}
- cluster_name:
    value: ${aws_eks_cluster.eks_cluster.name}
- oidc_provider_arn:
    value: ${replace(aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer, "https://",
      "")}
- cluster_oidc_provider_url:
    value: ${aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer}
- cluster_oidc_provider_arn:
    value: ${aws_iam_openid_connect_provider.identity_provider[0].arn}
---
variable:
- vpc_name: {}
- vpc_id:
    default: ''
- csoc_account_id:
    default: '433568766270'
- csoc_managed:
    default: false
- ec2_keyname:
    default: someone@uchicago.edu
- instance_type:
    default: t3.large
- jupyter_instance_type:
    default: t3.large
- workflow_instance_type:
    default: t3.2xlarge
- peering_cidr:
    default: 10.128.0.0/20
- secondary_cidr_block:
    default: ''
- peering_vpc_id:
    default: vpc-e2b51d99
- users_policy: {}
- worker_drive_size:
    default: 30
- eks_version:
    default: '1.25'
- workers_subnet_size:
    default: 24
- kernel:
    default: N/A
- bootstrap_script:
    default: bootstrap.sh
- jupyter_bootstrap_script:
    default: bootstrap.sh
- jupyter_worker_drive_size:
    default: 30
- workflow_bootstrap_script:
    default: bootstrap.sh
- workflow_worker_drive_size:
    default: 30
- cidrs_to_route_to_gw:
    default: []
- organization_name:
    default: Basic Services
- proxy_name:
    default: ' HTTP Proxy'
- jupyter_asg_desired_capacity:
    default: 0
- jupyter_asg_max_size:
    default: 10
- jupyter_asg_min_size:
    default: 0
- workflow_asg_desired_capacity:
    default: 0
- workflow_asg_max_size:
    default: 50
- workflow_asg_min_size:
    default: 0
- iam-serviceaccount:
    default: false
- oidc_eks_thumbprint:
    default:
    - 9e99a48a9960b14926bb7f3b02e22da2b0ab7280
    description: Thumbprint for the AWS OIDC identity provider
- availability_zones:
    default:
    - us-east-1a
    - us-east-1c
    - us-east-1d
    description: AZ to be used by EKS nodes
- domain_test:
    default: www.google.com
    description: Domain for the lambda function to check for the proxy
- ha_squid:
    default: false
    description: Is HA squid deployed?
- deploy_workflow:
    default: false
    description: Deploy workflow nodepool?
- secondary_availability_zones:
    default:
    - us-east-1a
    - us-east-1b
    - us-east-1c
    - us-east-1d
    description: AZ to be used by EKS nodes in the secondary subnet
- deploy_jupyter:
    default: true
    description: Deploy workflow nodepool?
- dual_proxy:
    description: Single instance and HA
- single_az_for_jupyter:
    default: false
    description: Jupyter notebooks on a single AZ
- sns_topic_arn:
    default: arn:aws:sns:us-east-1:433568766270:planx-csoc-alerts-topic
    description: SNS topic ARN for alerts
- activation_id:
    default: ''
- customer_id:
    default: ''
- fips:
    default: false
- fips_ami_kms:
    default: arn:aws:kms:us-east-1:707767160287:key/mrk-697897f040ef45b0aa3cebf38a916f99
- fips_enabled_ami:
    default: ami-0de87e3680dcb13ec
- use_asg:
    default: true
- use_karpenter:
    default: false
- deploy_karpenter_in_k8s:
    default: false
    description: Allows you to enable the Karpenter Helm chart and associated resources
      without deploying the other parts of karpenter (i.e. the roles, permissions,
      and SQS queue)
- karpenter_version:
    default: v0.32.9
- spot_linked_role:
    default: false
- scale_in_protection:
    default: false
    description: set scale-in protection on ASG
- ci_run:
    default: false
- eks_public_access:
    default: 'true'
- enable_vpc_endpoints:
    default: true
- k8s_bootstrap_resources:
    default: false
    description: If set to true, creates resources for bootstrapping a kubernetes
      cluster (such as karpenter configs and helm releases)
