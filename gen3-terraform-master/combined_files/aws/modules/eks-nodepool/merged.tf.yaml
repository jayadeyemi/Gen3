locals:
- vpc_id: '${var.vpc_id != "" ? var.vpc_id : data.aws_vpc.the_vpc.id}'
resource:
- aws_iam_role:
    eks_control_plane_role:
      assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n\
        \    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n  \
        \      \"Service\": \"eks.amazonaws.com\"\n      },\n      \"Effect\": \"\
        Allow\",\n      \"Sid\": \"\"\n    }\n  ]\n}"
      name: ${var.vpc_name}_EKS_${var.nodepool}_role
- aws_iam_role_policy_attachment:
    eks-policy-AmazonEKSClusterPolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
      role: ${aws_iam_role.eks_control_plane_role.name}
- aws_iam_role_policy_attachment:
    eks-policy-AmazonEKSServicePolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSServicePolicy
      role: ${aws_iam_role.eks_control_plane_role.name}
- aws_iam_role_policy_attachment:
    eks-policy-AmazonSSMManagedInstanceCore:
      policy_arn: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      role: ${aws_iam_role.eks_control_plane_role.name}
- aws_iam_role:
    eks_node_role:
      assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n\
        \    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\"\
        : \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n\
        \    }\n  ]\n}"
      name: eks_${var.vpc_name}_nodepool_${var.nodepool}_role
- aws_iam_policy:
    cwl_access_policy:
      description: In order to avoid the creation of users and keys, we are using
        roles and policies.
      name: ${var.vpc_name}_EKS_nodepool_${var.nodepool}_access_to_cloudwatchlogs
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n       \
        \ {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:DescribeLogGroups\"\
        ,\n            \"Resource\": \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group::log-stream:*\"\
        \n        },\n        {\n            \"Effect\": \"Allow\",\n            \"\
        Action\": [\n                \"logs:CreateLogStream\",\n                \"\
        logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\n      \
        \      ],\n            \"Resource\": \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group:${var.vpc_name}:log-stream:*\"\
        \n        }\n    ]\n}"
- aws_iam_policy:
    access_to_kernels:
      description: To access custom Kernels
      name: ${var.vpc_name}_EKS_nodepool_${var.nodepool}_kernel_access
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n       \
        \ {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n    \
        \        \"Action\": [\n                \"s3:List*\",\n                \"\
        s3:Get*\"\n            ],\n            \"Resource\": [\n                \"\
        arn:aws:s3:::gen3-kernels/*\",\n                \"arn:aws:s3:::gen3-kernels\"\
        ,\n                \"arn:aws:s3:::qualys-agentpackage\",\n               \
        \ \"arn:aws:s3:::qualys-agentpackage/*\"\n            ]\n        }\n    ]\n\
        }"
- aws_iam_policy:
    asg_access:
      description: Allow the deployment cluster-autoscaler to add or terminate instances
        accordingly
      name: ${var.vpc_name}_EKS_nodepool_${var.nodepool}_autoscaling_access
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n       \
        \ {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n     \
        \           \"autoscaling:DescribeAutoScalingGroups\",\n                \"\
        autoscaling:DescribeAutoScalingInstances\",\n                \"autoscaling:DescribeTags\"\
        ,\n                \"autoscaling:SetDesiredCapacity\",\n                \"\
        autoscaling:TerminateInstanceInAutoScalingGroup\",\n                \"autoscaling:DescribeLaunchConfigurations\"\
        \n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}"
- aws_iam_role_policy_attachment:
    eks-node-AmazonEKSWorkerNodePolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEKS_CNI_Policy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEKSCSIDriverPolicy:
      policy_arn: arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    eks-node-AmazonEC2ContainerRegistryReadOnly:
      policy_arn: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    cloudwatch_logs_access:
      policy_arn: ${aws_iam_policy.cwl_access_policy.arn}
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    asg_access:
      policy_arn: ${aws_iam_policy.asg_access.arn}
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_role_policy_attachment:
    kernel_access:
      policy_arn: ${aws_iam_policy.access_to_kernels.arn}
      role: ${aws_iam_role.eks_node_role.name}
- aws_iam_instance_profile:
    eks_node_instance_profile:
      name: ${var.vpc_name}_EKS_nodepool_${var.nodepool}
      role: ${aws_iam_role.eks_node_role.name}
- aws_security_group:
    eks_nodes_sg:
      description: 'Security group for all nodes in pool ${var.nodepool} in the EKS
        cluster [${var.vpc_name}] '
      egress:
      - cidr_blocks:
        - 0.0.0.0/0
        from_port: 0
        protocol: '-1'
        to_port: 0
      name: ${var.vpc_name}_EKS_nodepool_${var.nodepool}_sg
      tags: '${tomap({"Name": "${var.vpc_name}-nodes-sg-${var.nodepool}", "kubernetes.io/cluster/${var.vpc_name}":
        "owned", "karpenter.sh/discovery": "${var.vpc_name}-${var.nodepool}"})}'
      vpc_id: ${local.vpc_id}
- aws_security_group_rule:
    https_nodes_to_plane:
      depends_on:
      - ${aws_security_group.eks_nodes_sg}
      from_port: 443
      protocol: tcp
      security_group_id: ${var.control_plane_sg}
      source_security_group_id: ${aws_security_group.eks_nodes_sg.id}
      to_port: 443
      type: ingress
- aws_security_group_rule:
    communication_plane_to_nodes:
      depends_on:
      - ${aws_security_group.eks_nodes_sg}
      from_port: 80
      protocol: tcp
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${var.control_plane_sg}
      to_port: 65534
      type: ingress
- aws_security_group_rule:
    nodes_internode_communications:
      description: allow nodes to communicate with each other
      from_port: 0
      protocol: '-1'
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      self: true
      to_port: 0
      type: ingress
- aws_security_group_rule:
    nodes_interpool_communications:
      description: allow default nodes to communicate with each other
      from_port: 0
      protocol: '-1'
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${var.default_nodepool_sg}
      to_port: 0
      type: ingress
- aws_launch_template:
    eks_launch_template:
      block_device_mappings:
      - device_name: /dev/xvda
        ebs:
        - volume_size: ${var.nodepool_worker_drive_size}
      iam_instance_profile:
      - name: ${aws_iam_instance_profile.eks_node_instance_profile.name}
      image_id: ${data.aws_ami.eks_worker.id}
      instance_type: ${var.nodepool_instance_type}
      key_name: ${var.ec2_keyname}
      lifecycle:
      - create_before_destroy: true
      name_prefix: eks-${var.vpc_name}-nodepool-${var.nodepool}
      network_interfaces:
      - associate_public_ip_address: false
        security_groups:
        - ${aws_security_group.eks_nodes_sg.id}
        - ${aws_security_group.ssh.id}
      tag_specifications:
      - resource_type: instance
        tags:
          Name: eks-${var.vpc_name}-${var.nodepool}
      user_data: '${sensitive(base64encode(templatefile("${path.module}/../../../../flavors/eks/${var.bootstrap_script}",
        {"eks_ca": "${var.eks_cluster_ca}", "eks_endpoint": "${var.eks_cluster_endpoint}",
        "eks_region": "${data.aws_region.current.name}", "vpc_name": "${var.vpc_name}",
        "ssh_keys": "${templatefile("${path.module}/../../../../files/authorized_keys/ops_team",
        {})}", "nodepool": "${var.nodepool}", "lifecycle_type": "ONDEMAND", "kernel":
        "${var.kernel}", "activation_id": "${var.activation_id}", "customer_id": "${var.customer_id}"})))}'
- aws_autoscaling_group:
    eks_autoscaling_group:
      desired_capacity: ${var.nodepool_asg_desired_capacity}
      launch_template:
      - id: ${aws_launch_template.eks_launch_template.id}
        version: $Latest
      lifecycle:
      - ignore_changes:
        - ${desired_capacity}
      max_size: ${var.nodepool_asg_max_size}
      min_size: ${var.nodepool_asg_min_size}
      name: eks-${var.nodepool}worker-node-${var.vpc_name}
      protect_from_scale_in: ${var.scale_in_protection}
      tag:
      - key: Environment
        propagate_at_launch: true
        value: ${var.vpc_name}
      - key: Name
        propagate_at_launch: true
        value: eks-${var.vpc_name}-${var.nodepool}
      - key: kubernetes.io/cluster/${var.vpc_name}
        propagate_at_launch: true
        value: owned
      - key: k8s.io/cluster-autoscaler/enabled
        propagate_at_launch: true
        value: ''
      - key: k8s.io/cluster-type/eks
        propagate_at_launch: true
        value: ''
      - key: k8s.io/nodepool/${var.nodepool}
        propagate_at_launch: true
        value: ''
      - key: k8s.io/cluster-autoscaler/node-template/label/role
        propagate_at_launch: true
        value: ${var.nodepool}
      - key: k8s.io/cluster-autoscaler/node-template/taint/role
        propagate_at_launch: true
        value: ${var.nodepool}:NoSchedule
      vpc_zone_identifier: ${flatten([${var.eks_private_subnets}])}
- aws_security_group:
    ssh:
      description: security group that only enables ssh
      ingress:
      - cidr_blocks:
        - 0.0.0.0/0
        from_port: 22
        protocol: TCP
        to_port: 22
      name: ssh_eks_${var.vpc_name}-nodepool-${var.nodepool}
      tags:
        Environment: ${var.vpc_name}
        Name: ssh_eks_${var.vpc_name}-nodepool-${var.nodepool}
        Organization: ${var.organization_name}
        karpenter.sh/discovery: ${var.vpc_name}-${var.nodepool}
      vpc_id: ${local.vpc_id}
---
data:
- aws_caller_identity:
    current: {}
- aws_region:
    current: {}
- aws_vpcs:
    vpcs:
      tags:
        Name: ${var.vpc_name}
- aws_vpc:
    the_vpc:
      id: ${data.aws_vpcs.vpcs.ids[0]}
- aws_availability_zones:
    available:
      state: available
- aws_ami:
    eks_worker:
      filter:
      - name: name
        values:
        - amazon-eks-node-${var.eks_version}*
      most_recent: true
      owners:
      - '602401143452'
---
output:
- nodepool_role:
    value: ${aws_iam_role.eks_node_role.arn}
- nodepool_sg:
    value: ${aws_security_group.eks_nodes_sg.id}
---
variable:
- vpc_name: {}
- vpc_id:
    default: ''
- ec2_keyname:
    default: someone@uchicago.edu
- instance_type:
    default: t3.large
- nodepool_instance_type:
    default: t3.large
- csoc_cidr:
    default: 10.128.0.0/20
- users_policy: {}
- nodepool:
    default: jupyter
- eks_cluster_ca:
    default: ''
- eks_cluster_endpoint:
    default: ''
- eks_private_subnets: {}
- control_plane_sg: {}
- default_nodepool_sg: {}
- deploy_nodepool_pool:
    default: 'no'
- eks_version: {}
- kernel:
    default: N/A
- bootstrap_script:
    default: bootstrap-2.0.0.sh
- nodepool_worker_drive_size:
    default: 30
- organization_name:
    default: Basic Service
- nodepool_asg_desired_capacity:
    default: 0
- nodepool_asg_max_size:
    default: 10
- nodepool_asg_min_size:
    default: 0
- activation_id:
    default: ''
- customer_id:
    default: ''
- scale_in_protection:
    default: false
    description: set scale-in protection on ASG
