---
resource:
  aws_iam_role:
    eks_control_plane_role:
      name: ${var.vpc_name}_EKS_role
      assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n        \"Service\": \"eks.amazonaws.com\"\n      },\n      \"Effect\": \"Allow\",\n      \"Sid\": \"\"\n    }\n  ]\n}"
  aws_iam_role_policy_attachment:
    eks-policy-AmazonEKSClusterPolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
      role: ${aws_iam_role.eks_control_plane_role.name}
  aws_iam_role_policy_attachment:
    eks-policy-AmazonEKSServicePolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSServicePolicy
      role: ${aws_iam_role.eks_control_plane_role.name}
  aws_iam_role_policy_attachment:
    bucket_write:
      policy_arn: arn:aws:iam::${data.aws_caller_identity.current.account_id}:policy/bucket_writer_logs-${var.vpc_name}-gen3
      role: ${aws_iam_role.eks_control_plane_role.name}
  random_shuffle:
    az:
      input:
        ${local.azs}
      result_count: ${length(local.azs)}
      count: 1
  random_shuffle:
    secondary_az:
      input:
        ${local.secondary_azs}
      result_count: ${length(local.secondary_azs)}
      count: 1
  aws_subnet:
    eks_private:
      count: ${random_shuffle.az.result_count}
      vpc_id: ${data.aws_vpc.the_vpc.id}
      cidr_block: '${var.workers_subnet_size == 22 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 2 , ( 1 + count.index )) : var.workers_subnet_size == 23 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 3 , ( 2 + count.index )) : cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 4 , ( 7 + count.index )) }'
      availability_zone: ${random_shuffle.az.result[count.index]}
      map_public_ip_on_launch: false
      tags: "${\n    map(\n     \"Name\", \"eks_private_${count.index}\",\n     \"Environment\", \"${var.vpc_name}\",\n     \"Organization\", \"${var.organization_name}\",\n     \"kubernetes.io/cluster/${var.vpc_name}\", \"owned\",\n     \"kubernetes.io/role/internal-elb\", \"1\",\n    )\n  }"
      lifecycle:
        ignore_changes:
          tags
          availability_zone
  aws_subnet:
    eks_secondary_subnet:
      count: '${var.secondary_cidr_block != "" ? 4 : 0}'
      vpc_id: ${data.aws_vpc.the_vpc.id}
      cidr_block: ${cidrsubnet(var.secondary_cidr_block, 2 , count.index)}
      availability_zone: ${random_shuffle.secondary_az.result[count.index]}
      map_public_ip_on_launch: false
      tags: "${\n    map(\n     \"Name\", \"eks_secondary_cidr_subnet_${count.index}\",\n     \"Environment\", \"${var.vpc_name}\",\n     \"Organization\", \"${var.organization_name}\",\n     \"kubernetes.io/cluster/${var.vpc_name}\", \"owned\",\n     \"kubernetes.io/role/internal-elb\", \"1\",\n    )\n  }"
      lifecycle:
        ignore_changes:
          tags
          availability_zone
  aws_subnet:
    eks_public:
      count: ${random_shuffle.az.result_count}
      vpc_id: ${data.aws_vpc.the_vpc.id}
      cidr_block: '${var.workers_subnet_size == 22 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 5 , ( 4 + count.index )) : cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 4 , ( 10 + count.index ))}'
      map_public_ip_on_launch: true
      availability_zone: ${random_shuffle.az.result[count.index]}
      tags: "${\n    map(\n     \"Name\", \"eks_public_${count.index}\",\n     \"Environment\", \"${var.vpc_name}\",\n     \"Organization\", \"${var.organization_name}\",\n     \"kubernetes.io/cluster/${var.vpc_name}\", \"shared\",\n     \"kubernetes.io/role/elb\", \"1\",\n     \"KubernetesCluster\", \"${var.vpc_name}\",\n    )\n  }"
      lifecycle:
        ignore_changes:
          tags
          availability_zone
  aws_route_table:
    eks_private:
      vpc_id: ${data.aws_vpc.the_vpc.id}
      tags:
        Name: eks_private
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      lifecycle:
        {}
  aws_route:
    for_peering:
      route_table_id: ${aws_route_table.eks_private.id}
      destination_cidr_block: ${var.peering_cidr}
      vpc_peering_connection_id: ${data.aws_vpc_peering_connection.pc.id}
  aws_route:
    skip_proxy:
      count: ${length(var.cidrs_to_route_to_gw)}
      route_table_id: ${aws_route_table.eks_private.id}
      destination_cidr_block: ${element(var.cidrs_to_route_to_gw,count.index)}
      nat_gateway_id: ${data.aws_nat_gateway.the_gateway.id}
      depends_on:
        aws_route_table.eks_private
  aws_route:
    public_access:
      count: '${var.ha_squid ? var.dual_proxy ? 1 : 0 : 1}'
      destination_cidr_block: 0.0.0.0/0
      route_table_id: ${aws_route_table.eks_private.id}
      instance_id: ${data.aws_instances.squid_proxy.ids[0]}
  aws_route_table_association:
    private_kube:
      count: ${random_shuffle.az.result_count}
      subnet_id: ${aws_subnet.eks_private.*.id[count.index]}
      route_table_id: ${aws_route_table.eks_private.id}
      depends_on:
        aws_subnet.eks_private
  aws_route_table_association:
    secondary_subnet_kube:
      count: '${var.secondary_cidr_block != "" ? 1 : 0}'
      subnet_id: ${aws_subnet.eks_secondary_subnet.*.id[count.index]}
      route_table_id: ${aws_route_table.eks_private.id}
      depends_on:
        aws_subnet.eks_secondary_subnet
  aws_security_group:
    eks_control_plane_sg:
      name: ${var.vpc_name}-control-plane
      description: Cluster communication with worker nodes [${var.vpc_name}]
      vpc_id: ${data.aws_vpc.the_vpc.id}
      egress:
        from_port: 0
        to_port: 0
        protocol: '-1'
        cidr_blocks:
          0.0.0.0/0
      tags:
        Name: ${var.vpc_name}-control-plane-sg
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_route_table_association:
    public_kube:
      count: ${random_shuffle.az.result_count}
      subnet_id: ${aws_subnet.eks_public.*.id[count.index]}
      route_table_id: ${data.aws_route_table.public_kube.id}
      lifecycle:
        {}
  aws_eks_cluster:
    eks_cluster:
      name: ${var.vpc_name}
      role_arn: ${aws_iam_role.eks_control_plane_role.arn}
      version: ${var.eks_version}
      vpc_config:
        subnet_ids:
          ${aws_subnet.eks_private.*.id}
        security_group_ids:
          ${aws_security_group.eks_control_plane_sg.id}
        endpoint_private_access: 'true'
      depends_on:
        aws_iam_role_policy_attachment.eks-policy-AmazonEKSClusterPolicy
        aws_iam_role_policy_attachment.eks-policy-AmazonEKSServicePolicy
        aws_subnet.eks_private
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_iam_role:
    eks_node_role:
      name: eks_${var.vpc_name}_workers_role
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}"
  aws_iam_policy:
    cwl_access_policy:
      name: ${var.vpc_name}_EKS_workers_access_to_cloudwatchlogs
      description: In order to avoid the creation of users and keys, we are using roles and policies.
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:DescribeLogGroups\",\n            \"Resource\": \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group::log-stream:*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\n            ],\n            \"Resource\": \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group:${var.vpc_name}:log-stream:*\"\n        }\n    ]\n}"
  aws_iam_policy:
    asg_access:
      name: ${var.vpc_name}_EKS_workers_autoscaling_access
      description: Allow the deployment cluster-autoscaler to add or terminate instances accordingly
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"autoscaling:DescribeAutoScalingGroups\",\n                \"autoscaling:DescribeAutoScalingInstances\",\n                \"autoscaling:DescribeTags\",\n                \"autoscaling:SetDesiredCapacity\",\n                \"autoscaling:TerminateInstanceInAutoScalingGroup\",\n                \"autoscaling:DescribeLaunchConfigurations\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"ec2:CreateTags\",\n            \"Resource\": \"arn:aws:ec2:*:${data.aws_caller_identity.current.account_id}:instance/*\"\n        }\n    ]\n}"
  aws_iam_role_policy:
    csoc_alert_sns_access:
      count: '${var.sns_topic_arn != "" ? 1 : 0}'
      name: ${var.vpc_name}_CSOC_alert_SNS_topic_acess
      policy: ${data.aws_iam_policy_document.planx-csoc-alerts-topic_access.json}
      role: ${aws_iam_role.eks_node_role.id}
  aws_iam_role_policy_attachment:
    eks-node-AmazonEKSWorkerNodePolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    eks-node-AmazonEKS_CNI_Policy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    eks-node-AmazonEC2ContainerRegistryReadOnly:
      policy_arn: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    cloudwatch_logs_access:
      policy_arn: ${aws_iam_policy.cwl_access_policy.arn}
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    asg_access:
      policy_arn: ${aws_iam_policy.asg_access.arn}
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    bucket_read:
      policy_arn: arn:aws:iam::${data.aws_caller_identity.current.account_id}:policy/bucket_reader_cdis-gen3-users_${var.users_policy}
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    eks-policy-AmazonSSMManagedInstanceCore:
      policy_arn: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_instance_profile:
    eks_node_instance_profile:
      name: ${var.vpc_name}_EKS_workers
      role: ${aws_iam_role.eks_node_role.name}
  aws_security_group:
    eks_nodes_sg:
      name: ${var.vpc_name}_EKS_workers_sg
      description: 'Security group for all nodes in the EKS cluster [${var.vpc_name}] '
      vpc_id: ${data.aws_vpc.the_vpc.id}
      egress:
        from_port: 0
        to_port: 0
        protocol: '-1'
        cidr_blocks:
          0.0.0.0/0
      tags: "${\n    map(\n     \"Name\", \"${var.vpc_name}-nodes-sg\",\n     \"kubernetes.io/cluster/${var.vpc_name}\", \"owned\",\n    )\n  }"
  aws_security_group_rule:
    https_nodes_to_plane:
      type: ingress
      from_port: 443
      to_port: 443
      protocol: tcp
      security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      source_security_group_id: ${aws_security_group.eks_nodes_sg.id}
      depends_on:
        aws_security_group.eks_nodes_sg
        aws_security_group.eks_control_plane_sg
      description: from the workers to the control plane
  aws_security_group_rule:
    communication_plane_to_nodes:
      type: ingress
      from_port: 80
      to_port: 65534
      protocol: tcp
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      depends_on:
        aws_security_group.eks_nodes_sg
        aws_security_group.eks_control_plane_sg
      description: from the control plane to the nodes
  aws_security_group_rule:
    nodes_internode_communications:
      type: ingress
      from_port: 0
      to_port: 0
      protocol: '-1'
      description: allow nodes to communicate with each other
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      self: true
  aws_security_group_rule:
    nodes_interpool_communications:
      type: ingress
      from_port: 0
      to_port: 0
      protocol: '-1'
      description: allow jupyter nodes to talk to the default
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${module.jupyter_pool.nodepool_sg}
  aws_security_group_rule:
    workflow_nodes_interpool_communications:
      type: ingress
      from_port: 0
      to_port: 0
      protocol: '-1'
      description: allow workflow nodes to talk to the default
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${module.workflow_pool.nodepool_sg}
  aws_launch_configuration:
    eks_launch_configuration:
      associate_public_ip_address: false
      iam_instance_profile: ${aws_iam_instance_profile.eks_node_instance_profile.name}
      image_id: ${local.ami}
      instance_type: ${var.instance_type}
      name_prefix: eks-${var.vpc_name}
      security_groups:
        ${aws_security_group.eks_nodes_sg.id}
        ${aws_security_group.ssh.id}
      user_data_base64: ${base64encode(data.template_file.bootstrap.rendered)}
      key_name: ${var.ec2_keyname}
      root_block_device:
        volume_size: ${var.worker_drive_size}
      lifecycle:
        create_before_destroy: true
  aws_iam_service_linked_role:
    autoscaling:
      aws_service_name: autoscaling.amazonaws.com
      custom_suffix: ${var.vpc_name}
  aws_kms_grant:
    kms:
      count: '${var.fips ? 1 : 0}'
      name: kms-cmk-eks
      key_id: ${var.fips_ami_kms}
      grantee_principal: ${aws_iam_service_linked_role.autoscaling.arn}
      operations:
        Encrypt
        Decrypt
        ReEncryptFrom
        ReEncryptTo
        GenerateDataKey
        GenerateDataKeyWithoutPlaintext
        DescribeKey
        CreateGrant
  aws_autoscaling_group:
    eks_autoscaling_group:
      service_linked_role_arn: ${aws_iam_service_linked_role.autoscaling.arn}
      desired_capacity: 2
      launch_configuration: ${aws_launch_configuration.eks_launch_configuration.id}
      max_size: 10
      min_size: 2
      name: eks-worker-node-${var.vpc_name}
      vpc_zone_identifier:
        ${aws_subnet.eks_private.*.id}
      tag:
        key: Environment
        value: ${var.vpc_name}
        propagate_at_launch: true
        key: Name
        value: eks-${var.vpc_name}
        propagate_at_launch: true
        key: kubernetes.io/cluster/${var.vpc_name}
        value: owned
        propagate_at_launch: true
        key: k8s.io/cluster-autoscaler/enabled
        value: ''
        propagate_at_launch: true
        key: k8s.io/cluster-type/eks
        value: ''
        propagate_at_launch: true
        key: k8s.io/nodepool/default
        value: ''
        propagate_at_launch: true
      lifecycle:
        ignore_changes:
          desired_capacity
          max_size
          min_size
  aws_security_group:
    ssh:
      name: ssh_eks_${var.vpc_name}
      description: security group that only enables ssh
      vpc_id: ${data.aws_vpc.the_vpc.id}
      ingress:
        from_port: 22
        to_port: 22
        protocol: TCP
        cidr_blocks:
          0.0.0.0/0
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
        Name: ssh_eks_${var.vpc_name}
  null_resource:
    config_setup:
      triggers:
        kubeconfig_change: ${data.template_file.kube_config.rendered}
        configmap_change: '{local.config-map-aws-auth}'
      provisioner:
        local-exec:
          command: mkdir -p ${var.vpc_name}_output_EKS; echo '${data.template_file.kube_config.rendered}' >${var.vpc_name}_output_EKS/kubeconfig
        local-exec:
          command: echo "${local.config-map-aws-auth}" > ${var.vpc_name}_output_EKS/aws-auth-cm.yaml
        local-exec:
          command: echo "${data.template_file.init_cluster.rendered}" > ${var.vpc_name}_output_EKS/init_cluster.sh
        local-exec:
          command: bash ${var.vpc_name}_output_EKS/init_cluster.sh
      depends_on:
        aws_autoscaling_group.eks_autoscaling_group
  aws_iam_openid_connect_provider:
    identity_provider:
      count: '${var.iam-serviceaccount ? var.eks_version == "1.12" ? 0 : 1 : 0}'
      url: ${aws_eks_cluster.eks_cluster.identity.0.oidc.0.issuer}
      client_id_list:
        sts.amazonaws.com
      thumbprint_list: ${var.oidc_eks_thumbprint}
      depends_on:
        aws_eks_cluster.eks_cluster
  aws_vpc_endpoint:
    ec2:
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.ec2.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids:
        ${aws_subnet.eks_private.*.id}
      tags:
        Name: to ec2
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_vpc_endpoint:
    sts:
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.sts.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids:
        ${aws_subnet.eks_private.*.id}
      tags:
        Name: to sts
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_vpc_endpoint:
    autoscaling:
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.autoscaling.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids:
        ${aws_subnet.eks_private.*.id}
      tags:
        Name: to autoscaling
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_vpc_endpoint:
    ecr-dkr:
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.ecr_dkr.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids:
        ${aws_subnet.eks_private.*.id}
      tags:
        Name: to ecr dkr
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_vpc_endpoint:
    ecr-api:
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.ecr_api.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids:
        ${aws_subnet.eks_private.*.id}
      tags:
        Name: to ecr api
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_vpc_endpoint:
    ebs:
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.ebs.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids:
        ${aws_subnet.eks_private.*.id}
      tags:
        Name: to ebs
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_vpc_endpoint:
    k8s-s3:
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: com.amazonaws.${data.aws_region.current.name}.s3
      route_table_ids:
        ${data.aws_route_table.public_kube.id}
        ${aws_route_table.eks_private.*.id}
      tags:
        Name: to s3
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      depends_on:
        aws_route_table.eks_private
  aws_vpc_endpoint:
    k8s-logs:
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.logs.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids:
        ${aws_subnet.eks_private.*.id}
      lifecycle:
        {}
      tags:
        Name: to cloudwatch logs
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_cloudwatch_log_group:
    gwl_group:
      count: '${var.ha_squid ? 1 : 0}'
      name: /aws/lambda/${var.vpc_name}-gw-checks-lambda
      retention_in_days: 14
  aws_iam_role_policy:
    lambda_policy_resources:
      count: '${var.ha_squid ? 1 : 0}'
      name: resources_acces
      policy: ${data.aws_iam_policy_document.with_resources.json}
      role: ${module.iam_role.role_id}
  aws_iam_role_policy:
    lambda_policy_no_resources:
      count: '${var.ha_squid ? 1 : 0}'
      name: no_resources_acces
      policy: ${data.aws_iam_policy_document.without_resources.json}
      role: ${module.iam_role.role_id}
  aws_iam_role_policy_attachment:
    lambda_logs:
      count: '${var.ha_squid ? 1 : 0}'
      role: ${module.iam_role.role_id}
      policy_arn: ${module.iam_policy.arn}
  aws_lambda_function:
    gw_checks:
      count: '${var.ha_squid ? 1 : 0}'
      filename: lambda_function_payload.zip
      function_name: ${var.vpc_name}-gw-checks-lambda
      role: ${module.iam_role.role_arn}
      handler: lambda_function.lambda_handler
      timeout: 45
      description: Checks for internet access from the worker nodes subnets
      vpc_config:
        subnet_ids:
          ${aws_subnet.eks_private.*.id}
        security_group_ids:
          ${aws_security_group.eks_nodes_sg.id}
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      source_code_hash: ${data.archive_file.lambda_function.output_base64sha256}
      runtime: python3.8
      environment:
        variables:
          vpc_name: ${var.vpc_name}
          domain_test: ${var.domain_test}
      depends_on:
        aws_cloudwatch_log_group.gwl_group
  aws_cloudwatch_event_rule:
    gw_checks_rule:
      count: '${var.ha_squid ? 1 : 0}'
      name: ${var.vpc_name}-GW-checks-job
      description: Check if the gateway is working every minute
      schedule_expression: rate(1 minute)
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_cloudwatch_event_target:
    cw_to_lambda:
      count: '${var.ha_squid ? 1 : 0}'
      rule: ${aws_cloudwatch_event_rule.gw_checks_rule.name}
      arn: ${aws_lambda_function.gw_checks.arn}
  aws_lambda_permission:
    allow_cloudwatch:
      count: '${var.ha_squid ? 1 : 0}'
      statement_id: AllowExecutionFromCloudWatch
      action: lambda:InvokeFunction
      function_name: ${aws_lambda_function.gw_checks.function_name}
      principal: events.amazonaws.com
      source_arn: ${aws_cloudwatch_event_rule.gw_checks_rule.arn}
