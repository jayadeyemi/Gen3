---
resource:
  aws_launch_template:
    eks_launch_template:
      name_prefix: eks-${var.vpc_name}
      image_id: ${local.ami}
      instance_type: ${var.instance_type}
      key_name: ${var.ec2_keyname}
      iam_instance_profile:
        name: ${aws_iam_instance_profile.eks_node_instance_profile.name}
      network_interfaces:
        associate_public_ip_address: false
        security_groups:
          ${aws_security_group.eks_nodes_sg.id}
          ${aws_security_group.ssh.id}
      user_data: '${sensitive(base64encode(templatefile("${path.module}/../../../../flavors/eks/${var.bootstrap_script}", {"eks_ca": "${aws_eks_cluster.eks_cluster.certificate_authority[0].data}", "eks_endpoint": "${aws_eks_cluster.eks_cluster.endpoint}", "eks_region": "${data.aws_region.current.name}", "vpc_name": "${var.vpc_name}", "ssh_keys": "${templatefile("${path.module}/../../../../files/authorized_keys/ops_team", {})}", "nodepool": "default", "lifecycle_type": "ONDEMAND", "activation_id": "${var.activation_id}", "customer_id": "${var.customer_id}"})))}'
      block_device_mappings:
        device_name: /dev/xvda
        ebs:
          volume_size: ${var.worker_drive_size}
      tag_specifications:
        resource_type: instance
        tags:
          Name: eks-${var.vpc_name}-node
      lifecycle:
        create_before_destroy: true
  aws_autoscaling_group:
    eks_autoscaling_group:
      count: '${var.use_asg ? 1 : 0}'
      service_linked_role_arn: ${aws_iam_service_linked_role.autoscaling.arn}
      desired_capacity: 2
      max_size: 10
      min_size: 2
      name: eks-worker-node-${var.vpc_name}
      vpc_zone_identifier: ${flatten([${aws_subnet.eks_private.*.id}])}
      launch_template:
        id: ${aws_launch_template.eks_launch_template.id}
        version: $Latest
      tag:
        key: Environment
        value: ${var.vpc_name}
        propagate_at_launch: true
        key: Name
        value: eks-${var.vpc_name}
        propagate_at_launch: true
        key: kubernetes.io/cluster/${var.vpc_name}
        value: owned
        propagate_at_launch: true
        key: k8s.io/cluster-autoscaler/enabled
        value: ''
        propagate_at_launch: true
        key: k8s.io/cluster-type/eks
        value: ''
        propagate_at_launch: true
        key: k8s.io/nodepool/default
        value: ''
        propagate_at_launch: true
      lifecycle:
        ignore_changes:
          ${desired_capacity}
          ${max_size}
          ${min_size}
  aws_iam_role:
    eks_control_plane_role:
      name: ${var.vpc_name}_EKS_role
      assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n        \"Service\": \"eks.amazonaws.com\"\n      },\n      \"Effect\": \"Allow\",\n      \"Sid\": \"\"\n    }\n  ]\n}"
  aws_iam_role_policy_attachment:
    eks-policy-AmazonEKSClusterPolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
      role: ${aws_iam_role.eks_control_plane_role.name}
  aws_iam_role_policy_attachment:
    eks-policy-AmazonEKSServicePolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSServicePolicy
      role: ${aws_iam_role.eks_control_plane_role.name}
  random_shuffle:
    az:
      count: 1
      input: ${local.azs}
      result_count: ${length(local.azs)}
  random_shuffle:
    secondary_az:
      count: 1
      input: ${local.secondary_azs}
      result_count: ${length(local.secondary_azs)}
  aws_subnet:
    eks_private:
      count: ${random_shuffle.az[0].result_count}
      vpc_id: ${local.vpc_id}
      cidr_block: '${var.workers_subnet_size == 22 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 2, (1 + count.index)) : var.workers_subnet_size == 23 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 3, (2 + count.index)) : cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 4, (7 + count.index))}'
      availability_zone: ${random_shuffle.az[0].result[count.index]}
      map_public_ip_on_launch: false
      lifecycle:
        ignore_changes:
          ${tags}
          ${availability_zone}
          ${cidr_block}
      tags: '${tomap({"Name": "eks_private_${count.index}", "Environment": "${var.vpc_name}", "Organization": "${var.organization_name}", "kubernetes.io/cluster/${var.vpc_name}": "owned", "kubernetes.io/role/internal-elb": "1", "karpenter.sh/discovery": "${var.vpc_name}"})}'
  aws_subnet:
    eks_secondary_subnet:
      count: '${var.secondary_cidr_block != "" ? 4 : 0}'
      vpc_id: ${local.vpc_id}
      cidr_block: ${cidrsubnet(var.secondary_cidr_block, 2, count.index)}
      availability_zone: ${random_shuffle.secondary_az[0].result[count.index]}
      map_public_ip_on_launch: false
      lifecycle:
        ignore_changes:
          ${tags}
          ${availability_zone}
          ${cidr_block}
      tags: '${tomap({"Name": "eks_secondary_cidr_subnet_${count.index}", "Environment": "${var.vpc_name}", "Organization": "${var.organization_name}", "kubernetes.io/cluster/${var.vpc_name}": "owned", "kubernetes.io/role/internal-elb": "1", "karpenter.sh/discovery": "${var.vpc_name}"})}'
  aws_subnet:
    eks_public:
      count: ${random_shuffle.az[0].result_count}
      vpc_id: ${local.vpc_id}
      cidr_block: '${var.workers_subnet_size == 22 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 5, (4 + count.index)) : cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 4, (10 + count.index))}'
      map_public_ip_on_launch: true
      availability_zone: ${random_shuffle.az[0].result[count.index]}
      lifecycle:
        ignore_changes:
          ${tags}
          ${availability_zone}
          ${cidr_block}
      tags: '${tomap({"Name": "eks_public_${count.index}", "Environment": "${var.vpc_name}", "Organization": "${var.organization_name}", "kubernetes.io/cluster/${var.vpc_name}": "shared", "kubernetes.io/role/elb": "1", "KubernetesCluster": "${var.vpc_name}"})}'
  aws_route_table:
    eks_private:
      vpc_id: ${local.vpc_id}
      lifecycle:
        {}
      tags:
        Name: eks_private
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_route:
    for_peering:
      route_table_id: ${aws_route_table.eks_private.id}
      destination_cidr_block: ${var.peering_cidr}
      vpc_peering_connection_id: ${data.aws_vpc_peering_connection.pc.id}
  aws_route:
    skip_proxy:
      count: ${length(var.cidrs_to_route_to_gw)}
      route_table_id: ${aws_route_table.eks_private.id}
      destination_cidr_block: ${element(var.cidrs_to_route_to_gw, count.index)}
      nat_gateway_id: ${data.aws_nat_gateway.the_gateway.id}
      depends_on:
        ${aws_route_table.eks_private}
  aws_route:
    public_access:
      count: '${var.ha_squid ? var.dual_proxy ? 1 : 0 : 1}'
      destination_cidr_block: 0.0.0.0/0
      route_table_id: ${aws_route_table.eks_private.id}
      network_interface_id: ${data.aws_instances.squid_proxy[count.index].ids[0]}
  aws_route_table_association:
    private_kube:
      count: ${random_shuffle.az[0].result_count}
      subnet_id: ${aws_subnet.eks_private.*.id[count.index]}
      route_table_id: ${aws_route_table.eks_private.id}
      depends_on:
        ${aws_subnet.eks_private}
  aws_route_table_association:
    secondary_subnet_kube:
      count: '${var.secondary_cidr_block != "" ? random_shuffle.secondary_az[0].result_count : 0}'
      subnet_id: ${aws_subnet.eks_secondary_subnet.*.id[count.index]}
      route_table_id: ${aws_route_table.eks_private.id}
      depends_on:
        ${aws_subnet.eks_secondary_subnet}
  aws_security_group:
    eks_control_plane_sg:
      name: ${var.vpc_name}-control-plane
      description: Cluster communication with worker nodes [${var.vpc_name}]
      vpc_id: ${local.vpc_id}
      egress:
        from_port: 0
        to_port: 0
        protocol: '-1'
        cidr_blocks:
          0.0.0.0/0
      tags:
        Name: ${var.vpc_name}-control-plane-sg
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_route_table_association:
    public_kube:
      count: ${random_shuffle.az[0].result_count}
      subnet_id: ${aws_subnet.eks_public.*.id[count.index]}
      route_table_id: ${data.aws_route_table.public_kube.id}
      lifecycle:
        {}
  aws_eks_cluster:
    eks_cluster:
      name: ${var.vpc_name}
      role_arn: ${aws_iam_role.eks_control_plane_role.arn}
      version: ${var.eks_version}
      vpc_config:
        subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
        security_group_ids:
          ${aws_security_group.eks_control_plane_sg.id}
        endpoint_private_access: 'true'
        endpoint_public_access: ${var.eks_public_access}
      depends_on:
        ${aws_iam_role_policy_attachment.eks-policy-AmazonEKSClusterPolicy}
        ${aws_iam_role_policy_attachment.eks-policy-AmazonEKSServicePolicy}
        ${aws_subnet.eks_private}
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_iam_role:
    eks_node_role:
      name: eks_${var.vpc_name}_workers_role
      assume_role_policy: "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}"
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_iam_policy:
    cwl_access_policy:
      name: ${var.vpc_name}_EKS_workers_access_to_cloudwatchlogs
      description: In order to avoid the creation of users and keys, we are using roles and policies.
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:DescribeLogGroups\",\n            \"Resource\": \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group::log-stream:*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\n            ],\n            \"Resource\": \"arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group:${var.vpc_name}:log-stream:*\"\n        }\n    ]\n}"
  aws_iam_policy:
    asg_access:
      name: ${var.vpc_name}_EKS_workers_autoscaling_access
      description: Allow the deployment cluster-autoscaler to add or terminate instances accordingly
      policy: "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"autoscaling:DescribeAutoScalingGroups\",\n                \"autoscaling:DescribeAutoScalingInstances\",\n                \"autoscaling:DescribeTags\",\n                \"autoscaling:SetDesiredCapacity\",\n                \"autoscaling:TerminateInstanceInAutoScalingGroup\",\n                \"autoscaling:DescribeLaunchConfigurations\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}"
  aws_iam_role_policy:
    csoc_alert_sns_access:
      count: '${var.sns_topic_arn != "" ? 1 : 0}'
      name: ${var.vpc_name}_CSOC_alert_SNS_topic_acess
      policy: ${data.aws_iam_policy_document.planx-csoc-alerts-topic_access[count.index].json}
      role: ${aws_iam_role.eks_node_role.id}
  aws_iam_role_policy_attachment:
    eks-node-AmazonEKSWorkerNodePolicy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    eks-node-AmazonEKS_CNI_Policy:
      policy_arn: arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    eks-node-AmazonEKSCSIDriverPolicy:
      policy_arn: arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    eks-node-AmazonEC2ContainerRegistryReadOnly:
      policy_arn: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    cloudwatch_logs_access:
      policy_arn: ${aws_iam_policy.cwl_access_policy.arn}
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    asg_access:
      policy_arn: ${aws_iam_policy.asg_access.arn}
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    bucket_read:
      policy_arn: arn:aws:iam::${data.aws_caller_identity.current.account_id}:policy/bucket_reader_cdis-gen3-users_${var.users_policy}
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_role_policy_attachment:
    eks-policy-AmazonSSMManagedInstanceCore:
      policy_arn: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      role: ${aws_iam_role.eks_node_role.name}
  aws_iam_instance_profile:
    eks_node_instance_profile:
      name: ${var.vpc_name}_EKS_workers
      role: ${aws_iam_role.eks_node_role.name}
  aws_security_group:
    eks_nodes_sg:
      name: ${var.vpc_name}_EKS_workers_sg
      description: 'Security group for all nodes in the EKS cluster [${var.vpc_name}] '
      vpc_id: ${local.vpc_id}
      egress:
        from_port: 0
        to_port: 0
        protocol: '-1'
        cidr_blocks:
          0.0.0.0/0
      tags: '${tomap({"Name": "${var.vpc_name}-nodes-sg", "kubernetes.io/cluster/${var.vpc_name}": "owned", "karpenter.sh/discovery": "${var.vpc_name}"})}'
  aws_security_group_rule:
    https_nodes_to_plane:
      type: ingress
      from_port: 443
      to_port: 443
      protocol: tcp
      security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      source_security_group_id: ${aws_security_group.eks_nodes_sg.id}
      depends_on:
        ${aws_security_group.eks_nodes_sg}
        ${aws_security_group.eks_control_plane_sg}
      description: from the workers to the control plane
  aws_security_group_rule:
    https_csoc_to_plane:
      count: '${var.csoc_managed ? 1 : 0}'
      type: ingress
      from_port: 443
      to_port: 443
      protocol: tcp
      security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      cidr_blocks:
        ${var.peering_cidr}
      depends_on:
        ${aws_security_group.eks_nodes_sg}
        ${aws_security_group.eks_control_plane_sg}
      description: from the CSOC to the control plane
  aws_security_group_rule:
    communication_plane_to_nodes:
      type: ingress
      from_port: 80
      to_port: 65534
      protocol: tcp
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${aws_security_group.eks_control_plane_sg.id}
      depends_on:
        ${aws_security_group.eks_nodes_sg}
        ${aws_security_group.eks_control_plane_sg}
      description: from the control plane to the nodes
  aws_security_group_rule:
    nodes_internode_communications:
      type: ingress
      from_port: 0
      to_port: 0
      protocol: '-1'
      description: allow nodes to communicate with each other
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      self: true
  aws_security_group_rule:
    nodes_interpool_communications:
      count: '${var.deploy_jupyter ? 1 : 0}'
      type: ingress
      from_port: 0
      to_port: 0
      protocol: '-1'
      description: allow jupyter nodes to talk to the default
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${module.jupyter_pool[0].nodepool_sg}
  aws_security_group_rule:
    workflow_nodes_interpool_communications:
      count: '${var.deploy_workflow ? 1 : 0}'
      type: ingress
      from_port: 0
      to_port: 0
      protocol: '-1'
      description: allow workflow nodes to talk to the default
      security_group_id: ${aws_security_group.eks_nodes_sg.id}
      source_security_group_id: ${module.workflow_pool[0].nodepool_sg}
  aws_iam_service_linked_role:
    autoscaling:
      aws_service_name: autoscaling.amazonaws.com
      custom_suffix: ${var.vpc_name}
      lifecycle:
        ignore_changes:
          ${custom_suffix}
  aws_kms_grant:
    kms:
      count: '${var.fips ? 1 : 0}'
      name: kms-cmk-eks
      key_id: ${var.fips_ami_kms}
      grantee_principal: ${aws_iam_service_linked_role.autoscaling.arn}
      operations:
        Encrypt
        Decrypt
        ReEncryptFrom
        ReEncryptTo
        GenerateDataKey
        GenerateDataKeyWithoutPlaintext
        DescribeKey
        CreateGrant
  aws_security_group:
    ssh:
      name: ssh_eks_${var.vpc_name}
      description: security group that only enables ssh
      vpc_id: ${local.vpc_id}
      ingress:
        from_port: 22
        to_port: 22
        protocol: TCP
        cidr_blocks:
          0.0.0.0/0
      tags: '${tomap({"Environment": "${var.vpc_name}", "Organization": "${var.organization_name}", "Name": "ssh_eks_${var.vpc_name}", "karpenter.sh/discovery": "${var.vpc_name}"})}'
  kubectl_manifest:
    aws-auth:
      count: '${var.k8s_bootstrap_resources ? 1 : 0}'
      yaml_body: ${local.config-map-aws-auth}
  null_resource:
    config_setup:
      triggers:
        kubeconfig_change: '${sensitive(templatefile("${path.module}/kubeconfig.tpl", {"vpc_name": "${var.vpc_name}", "eks_name": "${aws_eks_cluster.eks_cluster.id}", "eks_endpoint": "${aws_eks_cluster.eks_cluster.endpoint}", "eks_cert": "${aws_eks_cluster.eks_cluster.certificate_authority[0].data}"}))}'
        configmap_change: ${sensitive(local.config-map-aws-auth)}
      provisioner:
        local-exec:
          command: mkdir -p ${var.vpc_name}_output_EKS; echo '${templatefile("${path.module}/kubeconfig.tpl", {vpc_name = var.vpc_name, eks_name = aws_eks_cluster.eks_cluster.id, eks_endpoint = aws_eks_cluster.eks_cluster.endpoint, eks_cert = aws_eks_cluster.eks_cluster.certificate_authority.0.data,})}' >${var.vpc_name}_output_EKS/kubeconfig
        local-exec:
          command: echo "${local.config-map-aws-auth}" > ${var.vpc_name}_output_EKS/aws-auth-cm.yaml
        local-exec:
          command: echo "${templatefile("${path.module}/init_cluster.sh", { vpc_name = var.vpc_name, kubeconfig_path = "${var.vpc_name}_output_EKS/kubeconfig", auth_configmap = "${var.vpc_name}_output_EKS/aws-auth-cm.yaml"})}" > ${var.vpc_name}_output_EKS/init_cluster.sh
      depends_on:
        ${aws_autoscaling_group.eks_autoscaling_group}
  aws_iam_openid_connect_provider:
    identity_provider:
      count: '${var.iam-serviceaccount ? var.eks_version == "1.12" ? 0 : 1 : 0}'
      url: ${aws_eks_cluster.eks_cluster.identity.0.oidc.0.issuer}
      client_id_list:
        sts.amazonaws.com
      thumbprint_list: ${var.oidc_eks_thumbprint}
      depends_on:
        ${aws_eks_cluster.eks_cluster}
  aws_vpc_endpoint:
    ec2:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.ec2.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Name: to ec2
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      lifecycle:
        ignore_changes: ${all}
  aws_vpc_endpoint:
    sts:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.sts.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Name: to sts
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      lifecycle:
        ignore_changes: ${all}
  aws_vpc_endpoint:
    autoscaling:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.autoscaling.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Name: to autoscaling
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      lifecycle:
        ignore_changes: ${all}
  aws_vpc_endpoint:
    ecr-dkr:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.ecr_dkr.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Name: to ecr dkr
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      lifecycle:
        ignore_changes: ${all}
  aws_vpc_endpoint:
    ecr-api:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.ecr_api.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Name: to ecr api
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      lifecycle:
        ignore_changes: ${all}
  aws_vpc_endpoint:
    ebs:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.ebs.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Name: to ebs
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      lifecycle:
        ignore_changes: ${all}
  aws_vpc_endpoint:
    k8s-s3:
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: com.amazonaws.${data.aws_region.current.name}.s3
      route_table_ids: ${flatten([${data.aws_route_table.public_kube.id}, ${aws_route_table.eks_private[*].id}])}
      depends_on:
        ${aws_route_table.eks_private}
      tags:
        Name: to s3
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      lifecycle:
        ignore_changes: ${all}
  aws_vpc_endpoint:
    k8s-logs:
      count: '${var.enable_vpc_endpoints ? 1 : 0}'
      vpc_id: ${data.aws_vpc.the_vpc.id}
      service_name: ${data.aws_vpc_endpoint_service.logs.service_name}
      vpc_endpoint_type: Interface
      security_group_ids:
        ${data.aws_security_group.local_traffic.id}
      private_dns_enabled: true
      subnet_ids: ${flatten([${aws_subnet.eks_private[*].id}])}
      tags:
        Name: to cloudwatch logs
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
      lifecycle:
        ignore_changes: ${all}
  aws_iam_role:
    irsa:
      count: '${var.use_karpenter ? 1 : 0}'
      name: ${local.irsa_name}
      assume_role_policy: ${data.aws_iam_policy_document.irsa_assume_role[0].json}
      force_detach_policies: true
  aws_iam_service_linked_role:
    ec2_spot:
      count: '${var.use_karpenter && var.spot_linked_role ? 1 : 0}'
      aws_service_name: spot.amazonaws.com
      description: Service-linked role for EC2 Spot Instances
  aws_iam_policy:
    irsa:
      count: '${var.use_karpenter ? 1 : 0}'
      name: ${local.irsa_policy_name}-policy
      policy: ${data.aws_iam_policy_document.irsa[0].json}
  aws_iam_role_policy_attachment:
    irsa:
      count: '${var.use_karpenter ? 1 : 0}'
      role: ${aws_iam_role.irsa[0].name}
      policy_arn: ${aws_iam_policy.irsa[0].arn}
  aws_sqs_queue:
    this:
      count: '${var.use_karpenter ? 1 : 0}'
      name: ${local.queue_name}
      message_retention_seconds: 300
  aws_sqs_queue_policy:
    this:
      count: '${var.use_karpenter ? 1 : 0}'
      queue_url: ${aws_sqs_queue.this[0].url}
      policy: ${data.aws_iam_policy_document.queue[0].json}
  aws_cloudwatch_event_rule:
    this:
      for_each: '${{for k , v in local.events : k => v if var.use_karpenter}}'
      name_prefix: ${var.vpc_name}-${each.value.name}-
      description: ${each.value.description}
      event_pattern: ${jsonencode(each.value.event_pattern)}
      tags:
        ClusterName: ${aws_eks_cluster.eks_cluster.id}
  aws_cloudwatch_event_target:
    this:
      for_each: '${{for k , v in local.events : k => v if var.use_karpenter}}'
      rule: ${aws_cloudwatch_event_rule.this[each.key].name}
      target_id: KarpenterInterruptionQueueTarget
      arn: ${aws_sqs_queue.this[0].arn}
  aws_eks_fargate_profile:
    karpenter:
      count: '${var.use_karpenter ? 1 : 0}'
      cluster_name: ${aws_eks_cluster.eks_cluster.name}
      fargate_profile_name: karpenter
      pod_execution_role_arn: ${aws_iam_role.karpenter[0].arn}
      subnet_ids: ${local.eks_priv_subnets}
      selector:
        namespace: karpenter
  time_sleep:
    wait_60_seconds:
      create_duration: 60s
      depends_on:
        ${aws_eks_fargate_profile.karpenter}
  aws_iam_role:
    karpenter:
      count: '${var.use_karpenter ? 1 : 0}'
      name: ${var.vpc_name}-karpenter-fargate-role
      assume_role_policy: '${jsonencode({"Statement": [{"Action": "sts:AssumeRole", "Effect": "Allow", "Principal": {"Service": "eks-fargate-pods.amazonaws.com"}}], "Version": "2012-10-17"})}'
  aws_iam_role_policy_attachment:
    karpenter-role-policy:
      count: '${var.use_karpenter ? 1 : 0}'
      policy_arn: arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy
      role: ${aws_iam_role.karpenter[0].name}
  helm_release:
    karpenter:
      count: '${var.k8s_bootstrap_resources && var.use_karpenter && var.deploy_karpenter_in_k8s ? 1 : 0}'
      namespace: karpenter
      create_namespace: true
      name: karpenter
      repository: oci://public.ecr.aws/karpenter
      chart: karpenter
      version: ${var.karpenter_version}
      set:
        name: settings.aws.clusterName
        value: ${aws_eks_cluster.eks_cluster.id}
        name: settings.aws.clusterEndpoint
        value: ${aws_eks_cluster.eks_cluster.endpoint}
        name: serviceAccount.annotations.eks\.amazonaws\.com/role-arn
        value: ${aws_iam_role.irsa[0].arn}
        name: settings.aws.defaultInstanceProfile
        value: ${aws_iam_instance_profile.eks_node_instance_profile.id}
        name: settings.aws.interruptionQueueName
        value: ${aws_sqs_queue.this[0].name}
        name: dnsPolicy
        value: Default
      depends_on:
        ${time_sleep.wait_60_seconds}
      lifecycle:
        ignore_changes: ${all}
  kubectl_manifest:
    karpenter_node_pool:
      count: '${var.k8s_bootstrap_resources && var.use_karpenter && var.deploy_karpenter_in_k8s ? 1 : 0}'
      yaml_body: "---\napiVersion: karpenter.sh/v1beta1\nkind: NodePool\nmetadata:\n  name: default\nspec:\n  disruption:\n    consolidateAfter: 30s\n    consolidationPolicy: WhenEmpty\n    expireAfter: \"168h\"\n  limits:\n    cpu: \"1000\"\n    memory: 1000Gi\n  template:\n    metadata:\n      labels:\n        role: default\n    spec:\n      kubelet:\n        evictionHard:\n          memory.available: 5%\n        evictionSoft:\n          memory.available: 10%\n        evictionSoftGracePeriod:\n          memory.available: 5m\n        kubeReserved:\n          cpu: 480m\n          ephemeral-storage: 3Gi\n          memory: 1632Mi\n      nodeClassRef:\n        apiVersion: karpenter.k8s.aws/v1beta1\n        kind: EC2NodeClass\n        name: default\n      requirements:\n      - key: karpenter.sh/capacity-type\n        operator: In\n        values:\n        - on-demand\n        - spot\n      - key: kubernetes.io/arch\n        operator: In\n        values:\n        - amd64\n      - key: karpenter.k8s.aws/instance-category\n        operator: In\n        values:\n        - c\n        - m\n        - r\n        - t"
      depends_on:
        ${helm_release.karpenter}
      lifecycle:
        ignore_changes: ${all}
  kubectl_manifest:
    karpenter_node_class:
      count: '${var.k8s_bootstrap_resources && var.use_karpenter && var.deploy_karpenter_in_k8s ? 1 : 0}'
      yaml_body: "    ---\n    apiVersion: karpenter.k8s.aws/v1beta1\n    kind: EC2NodeClass\n    metadata:\n      name: default\n    spec:\n      amiFamily: AL2\n      amiSelectorTerms:\n      - name: \"EKS-FIPS*\"\n        owner: \"143731057154\"\n      blockDeviceMappings:\n      - deviceName: /dev/xvda\n        ebs:\n          deleteOnTermination: true\n          encrypted: true\n          volumeSize: ${var.worker_drive_size}Gi\n          volumeType: gp3\n      metadataOptions:\n        httpEndpoint: enabled\n        httpProtocolIPv6: disabled\n        httpPutResponseHopLimit: 2\n        httpTokens: optional\n      role: eks_${var.vpc_name}_workers_role\n\n      securityGroupSelectorTerms:\n      - tags:\n          karpenter.sh/discovery: ${var.vpc_name}\n\n      subnetSelectorTerms:\n      - tags:\n          karpenter.sh/discovery: ${var.vpc_name}\n\n      tags:\n        Environment: ${var.vpc_name}\n        Name: eks-${var.vpc_name}-karpenter\n        karpenter.sh/discovery: ${var.vpc_name}\n        purpose: default\n\n      userData: |\n        MIME-Version: 1.0\n        Content-Type: multipart/mixed; boundary=\"BOUNDARY\"\n\n        --BOUNDARY\n        Content-Type: text/x-shellscript; charset=\"us-ascii\"\n\n        #!/bin/bash -x\n        instanceId=$(curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | jq -r .instanceId)\n        curl https://raw.githubusercontent.com/uc-cdis/cloud-automation/master/files/authorized_keys/ops_team >> /home/ec2-user/.ssh/authorized_keys\n        echo \"$(jq '.registryPullQPS=0' /etc/kubernetes/kubelet/kubelet-config.json)\" > /etc/kubernetes/kubelet/kubelet-config.json\n        sysctl -w fs.inotify.max_user_watches=12000\n\n        sudo yum update -y\n\n        --BOUNDARY--"
      depends_on:
        ${helm_release.karpenter}
      lifecycle:
        ignore_changes: ${all}
  aws_cloudwatch_log_group:
    gwl_group:
      count: '${var.ha_squid ? 1 : 0}'
      name: /aws/lambda/${var.vpc_name}-gw-checks-lambda
      retention_in_days: 14
  aws_iam_role_policy:
    lambda_policy_resources:
      count: '${var.ha_squid ? 1 : 0}'
      name: resources_acces
      policy: ${data.aws_iam_policy_document.with_resources.json}
      role: ${module.iam_role[0].role_id}
  aws_iam_role_policy:
    lambda_policy_no_resources:
      count: '${var.ha_squid ? 1 : 0}'
      name: no_resources_acces
      policy: ${data.aws_iam_policy_document.without_resources.json}
      role: ${module.iam_role[0].role_id}
  aws_iam_role_policy_attachment:
    lambda_logs:
      count: '${var.ha_squid ? 1 : 0}'
      role: ${module.iam_role[0].role_id}
      policy_arn: ${module.iam_policy[0].arn}
  aws_lambda_function:
    gw_checks:
      count: '${var.ha_squid ? 1 : 0}'
      filename: lambda_function_payload.zip
      function_name: ${var.vpc_name}-gw-checks-lambda
      role: ${module.iam_role[0].role_arn}
      handler: lambda_function.lambda_handler
      timeout: 45
      description: Checks for internet access from the worker nodes subnets
      depends_on:
        ${aws_cloudwatch_log_group.gwl_group}
      source_code_hash: ${data.archive_file.lambda_function.output_base64sha256}
      runtime: python3.8
      vpc_config:
        subnet_ids: ${flatten([${aws_subnet.eks_private.*.id}])}
        security_group_ids:
          ${aws_security_group.eks_nodes_sg.id}
      environment:
        variables:
          vpc_name: ${var.vpc_name}
          domain_test: ${var.domain_test}
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_cloudwatch_event_rule:
    gw_checks_rule:
      count: '${var.ha_squid ? 1 : 0}'
      name: ${var.vpc_name}-GW-checks-job
      description: Check if the gateway is working every minute
      schedule_expression: rate(1 minute)
      tags:
        Environment: ${var.vpc_name}
        Organization: ${var.organization_name}
  aws_cloudwatch_event_target:
    cw_to_lambda:
      count: '${var.ha_squid ? 1 : 0}'
      rule: ${aws_cloudwatch_event_rule.gw_checks_rule[count.index].name}
      arn: ${aws_lambda_function.gw_checks[count.index].arn}
  aws_lambda_permission:
    allow_cloudwatch:
      count: '${var.ha_squid ? 1 : 0}'
      statement_id: AllowExecutionFromCloudWatch
      action: lambda:InvokeFunction
      function_name: ${aws_lambda_function.gw_checks[count.index].function_name}
      principal: events.amazonaws.com
      source_arn: ${aws_cloudwatch_event_rule.gw_checks_rule[count.index].arn}
